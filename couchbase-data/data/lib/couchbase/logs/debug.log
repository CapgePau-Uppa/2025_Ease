[ns_server:info,2025-03-28T10:13:22.583Z,nonode@nohost:<0.155.0>:ns_server:init_logging:180]Started & configured logging
[ns_server:warn,2025-03-28T10:13:22.605Z,nonode@nohost:<0.155.0>:config_profile:load:123]Could not load profile file ("/etc/couchbase.d/config_profile") because it does not exist
[ns_server:debug,2025-03-28T10:13:22.608Z,nonode@nohost:<0.155.0>:ns_server:setup_server_profile:108]Using profile 'default': [{name,"default"},
                          {{indexer,disable_shard_affinity},true}]
[ns_server:warn,2025-03-28T10:13:22.611Z,nonode@nohost:<0.155.0>:ns_server:config_profile_continuity_checker:129]Writing config_profile '"default"' to disk.
[ns_server:info,2025-03-28T10:13:22.661Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,10},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_json_rpc,
     [{rotation,
          [{compress,true},
           {size,41943040},
           {num_files,2},
           {buffer_size_max,52428800}]}]},
 {disk_sink_opts_disk_tls_key_log,
     [{rotation,
          [{compress,true},
           {size,10485760},
           {num_files,1},
           {buffer_size_max,13107200}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2025-03-28T10:13:22.661Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.661Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.662Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.663Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.664Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.664Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_json_rpc, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.664Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter disk_sink_opts_disk_tls_key_log, which is given from command line
[ns_server:warn,2025-03-28T10:13:22.664Z,nonode@nohost:<0.155.0>:ns_server:log_pending:30]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2025-03-28T10:13:22.685Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2025-03-28T10:13:22.686Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-03-28T10:13:22.687Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:init:180]ip config not found. Looks like we're brand new node
[ns_server:info,2025-03-28T10:13:22.694Z,nonode@nohost:dist_manager<0.216.0>:dist_manager:bringup:246]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2025-03-28T10:13:22.731Z,nonode@nohost:ssl_dist_admin_sup<0.219.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.220.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:22.731Z,nonode@nohost:ssl_dist_admin_sup<0.219.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.221.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:22.731Z,nonode@nohost:ssl_dist_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.219.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.740Z,nonode@nohost:tls_dist_sup<0.222.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.223.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.746Z,nonode@nohost:tls_dist_server_sup<0.224.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.225.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.746Z,nonode@nohost:tls_dist_server_sup<0.224.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.226.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.746Z,nonode@nohost:tls_dist_server_sup<0.224.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.227.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.747Z,nonode@nohost:tls_dist_sup<0.222.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.224.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.747Z,nonode@nohost:ssl_dist_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.222.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:22.747Z,nonode@nohost:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.218.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:22.749Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Starting cb_dist with config [{config_vsn,2}]
[error_logger:info,2025-03-28T10:13:22.754Z,nonode@nohost:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.228.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:22.759Z,nonode@nohost:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.229.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:22.763Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2025-03-28T10:13:22.764Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2025-03-28T10:13:22.770Z,nonode@nohost:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:22.859Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated server ssl_dist_opts (keep_secrets) - false
[ns_server:debug,2025-03-28T10:13:22.859Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Started acceptor inet_tcp_dist: <0.232.0>
[ns_server:debug,2025-03-28T10:13:22.859Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[error_logger:info,2025-03-28T10:13:22.859Z,ns_1@cb.local:net_sup<0.217.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.230.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:22.860Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.217.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,60000},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:22.861Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:configure_net_kernel:302]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2025-03-28T10:13:22.864Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_node:160]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2025-03-28T10:13:22.888Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:bringup:269]Attempted to save node name to disk: ok
[ns_server:debug,2025-03-28T10:13:22.888Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:wait_for_node:276]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2025-03-28T10:13:22.888Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:22.894Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:22.894Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655937.12197>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:22.894Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655937.12197>,
                                  inet_tcp_dist,<0.234.0>,
                                  #Ref<0.3437536922.3327655938.10789>}
[ns_server:debug,2025-03-28T10:13:22.902Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:wait_for_node:288]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2025-03-28T10:13:22.903Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_address_config:147]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2025-03-28T10:13:22.904Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_address_config:148]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2025-03-28T10:13:22.924Z,ns_1@cb.local:dist_manager<0.216.0>:dist_manager:save_address_config:151]Persisted the address successfully
[error_logger:info,2025-03-28T10:13:22.932Z,ns_1@cb.local:root_sup<0.215.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.216.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:22.943Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.237.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:22.948Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {5,15,167}
Runtime info: [{otp_release,"25"},
               {erl_version,"13.2.2.3"},
               {erl_version_long,
                   "Erlang/OTP 25 [erts-13.2.2.3] [source-15104f9619] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:16] [jit:ns]\n"},
               {system_arch_raw,"x86_64-pc-linux-gnu"},
               {system_arch,"x86_64-pc-linux-gnu"},
               {localtime,{{2025,3,28},{10,13,22}}},
               {memory,
                   [{total,42872096},
                    {processes,10034848},
                    {processes_used,10031096},
                    {system,32837248},
                    {atom,540873},
                    {atom_used,522684},
                    {binary,267416},
                    {code,10771190},
                    {ets,2639768}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,inet_tcp,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ssl_dist_admin_sup,ssl_dist_sup,inet_tls_dist,
                    inet_tcp_dist,cb_epmd,gen_udp,inet_hosts,dist_manager,
                    root_sup,cb_dist,path_config,config_profile,
                    ns_server_stats,calendar,ale_default_formatter,
                    'ale_logger-tls_key','ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-ns_server_trace','ale_logger-menelaus',
                    'ale_logger-stats','ale_logger-json_rpc',
                    'ale_logger-access','ale_logger-ns_server',
                    'ale_logger-user','ale_logger-ns_doctor',
                    'ale_logger-cluster','ale_logger-xdcr',erl_bits,
                    otp_internal,cb_log_counter_sink,ns_log_sink,
                    ale_disk_sink,misc,couch_util,ns_server,esaml_util,esaml,
                    ale_error_logger_handler,timer,cpu_sup,filelib,memsup,
                    disksup,os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-trace_logger','ale_logger-ale_logger',
                    'ale_logger-error_logger',beam_opcodes,beam_dict,beam_asm,
                    beam_z,beam_flatten,beam_trim,beam_clean,beam_block,
                    beam_utils,beam_jump,beam_a,beam_validator,
                    beam_ssa_codegen,beam_ssa_pre_codegen,beam_ssa_throw,
                    beam_ssa_dead,beam_call_types,beam_types,beam_ssa_type,
                    beam_ssa_bc_size,beam_ssa_opt,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sys_core_fold,sys_core_inline,cerl_trees,
                    core_lib,cerl,sets,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,erpc,global_group,erl_distribution,maps,
                    rand,net_kernel,global,rpc,epp,inet_gethost_native,
                    inet_parse,inet,inet_udp,inet_config,inet_db,unicode,os,
                    gb_trees,gb_sets,binary,beam_lib,peer,erl_anno,
                    erl_features,proplists,erl_scan,queue,logger_olp,
                    logger_proxy,application,code,application_master,
                    error_logger,erl_eval,application_controller,file_server,
                    code_server,file_io_server,error_handler,gen_event,
                    logger_backend,heart,kernel,logger_config,erl_parse,file,
                    logger,logger_filters,proc_lib,logger_simple_h,supervisor,
                    logger_server,erl_lint,ets,gen,filename,gen_server,lists,
                    persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{stdlib,"ERTS  CXC 138 10","4.3.1.2"},
                    {ns_server,"Couchbase server","7.6.5-5704-enterprise"},
                    {inets,"INETS  CXC 138 49","8.3.1.2"},
                    {public_key,"Public key infrastructure","1.13.3.1"},
                    {crypto,"CRYPTO","5.1.4.1"},
                    {asn1,"The Erlang ASN1 compiler version 5.0.21","5.0.21"},
                    {ssl,"Erlang/OTP SSL application","10.9.1.2"},
                    {sasl,"SASL  CXC 138 11","4.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {esaml,"SAML Server Provider library for erlang","4.4.0"},
                    {os_mon,"CPO  CXC 138 46","2.8.2"},
                    {xmerl,"XML parser","1.3.31.1"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {kernel,"ERTS  CXC 138 10","8.5.4.2"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,159},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [lhttpc_sup,standard_error,net_kernel,'sink-disk_default',
                    kernel_refc,'sink-disk_stats',dtls_listener_sup,
                    ale_stats_events,esaml_ets_table_owner,ssl_dist_admin_sup,
                    tls_server_sup,ssl_connection_sup,'sink-disk_access_int',
                    erts_code_purger,ssl_upgrade_server_session_cache_sup,
                    erl_prim_loader,global_group,httpd_sup,
                    ssl_upgrade_server_session_cache_sup_dist,init,
                    application_controller,ale_sup,tls_client_ticket_store,
                    erl_signal_server,dtls_server_session_cache_sup,
                    logger_sup,tls_sup,dist_manager,ssl_pem_cache_dist,
                    inets_sup,'sink-disk_json_rpc',cb_dist,cpu_sup,ale,
                    'sink-disk_xdcr',ns_server_cluster_sup,httpc_manager,rex,
                    kernel_safe_sup,ssl_dist_sup,ssl_sup,esaml,
                    tls_dist_connection_sup,'sink-disk_access',
                    tls_server_session_ticket_sup_dist,ssl_listen_tracker_sup,
                    os_mon_sup,dtls_sup,logger_proxy,'sink-cb_log_counter',
                    tls_dist_sup,tls_server_session_ticket_sup,auth,disksup,
                    ssl_manager,'sink-disk_debug',kernel_sup,inet_db,user,
                    'sink-disk_trace',ssl_listen_tracker_sup_dist,
                    ale_dynamic_sup,'sink-disk_tls_key_log',
                    logger_std_h_ssl_handler,logger,standard_error_sup,
                    httpc_profile_sup,logger_handler_watcher,sasl_safe_sup,
                    ssl_admin_sup,release_handler,
                    ssl_server_session_cache_sup,'sink-disk_error',
                    global_name_server,file_server_2,root_sup,lhttpc_manager,
                    alarm_handler,'sink-disk_reports',httpc_sup,
                    dtls_server_sup,global_group_check,tls_dist_server_sup,
                    'sink-disk_metakv',local_tasks,ssl_pem_cache,memsup,
                    dtls_connection_sup,socket_registry,code_server,
                    httpc_handler_sup,sasl_sup,'sink-ns_log',net_sup,
                    ssl_manager_dist,tls_connection_sup]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,11}]
[ns_server:info,2025-03-28T10:13:22.958Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:log_os_info:start_link:21]Manifest:
["<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\" />",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"9ead6b88adbf8d6131e5ae7a3a699c477a3b4195\" groups=\"kv\" />",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"13238588ecf173dabad7f0b93e1abaaae90e27b1\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"91c822e00c1aa094688abc8b04dc2d1f0cc0ec21\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"198c313f4059edfcb9ce51ee5996c1b5c7bfc782\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"86d8fdd68842f94d1c61db36c10fe0cd1e75e85f\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"trinity\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"5704\" />",
 "    <annotation name=\"VERSION\" value=\"7.6.5\" />","  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"ea9d16d081777f878b8ec37b2d3be9080e640dfd\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"f16b7d1cd9e5d6ffbf7cb3e368d0d0c2e37b2ac2\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-ui\" revision=\"704db180d01de15f70cacc9fc11c5d8d8d4ff965\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"7e2c65d6c289a1eca62917d001000ced20948a4a\" groups=\"backup\" />",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"ac23deaa5d166ee122222320e6d09bcee65769b7\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"cbft\" revision=\"b49ac99c49d0917afd62edbb775a3c8eb0b5b849\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" />",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"7d906e2891e26432a6b5441666e5d69fea53015f\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"cbgt\" revision=\"b0b7b10995a9f569d00a21d7ea8005abd426ccb8\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"b93a39e6c46c3de1a73153d49579012f0737592b\" />",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"8d1feeb0d8b15e2b6a4c1a417addfd159b422a71\" />",
 "  <project name=\"client_golang\" path=\"godeps/src/github.com/prometheus/client_golang\" remote=\"couchbasedeps\" revision=\"2e1c4818ccfdcf953ce399cadad615ff2bed968c\" upstream=\"refs/tags/v1.12.1\" dest-branch=\"refs/tags/v1.12.1\" />",
 "  <project name=\"client_model\" path=\"godeps/src/github.com/prometheus/client_model\" remote=\"couchbasedeps\" revision=\"6dc836ede0b5b08c61893c3ffeb474498b18bb83\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\" />",
 "  <project name=\"common\" path=\"godeps/src/github.com/prometheus/common\" remote=\"couchbasedeps\" revision=\"902cb39e6c079571d32c2db8da220da13c11b562\" upstream=\"refs/tags/v0.33.0\" dest-branch=\"refs/tags/v0.33.0\" />",
 "  <project name=\"couchbase-cli\" revision=\"853137ffe96629a9fba24b765cf0c7200a7e16d1\" groups=\"kv\" />",
 "  <project name=\"couchdb\" revision=\"a7cd1de87bc486e953167bd61bc73e100067771a\" dest-branch=\"unstable\" />",
 "  <project name=\"couchdbx-app\" revision=\"5261b8844cea741087f36de52cecba8c2c8865da\" groups=\"notdefault,packaging\" />",
 "  <project name=\"couchstore\" revision=\"ce7305bab3feb64bd2504f34d24a1419008e8bda\" groups=\"kv\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"eb61739cd99fb244c7cd188d3c5bae54824e781d\" upstream=\"refs/tags/v0.15.0\" dest-branch=\"refs/tags/v0.15.0\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"cf3254d7dfb042192c9a23bd2e64a281c32a29d8\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"89e14e9640b37a2782e5e12e5536c3dd519058ca\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"b66aff60efa0fb95d5bba0e387eebb634f25d281\" dest-branch=\"unstable\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project name=\"forestdb\" revision=\"9efe6d75d7d61e742af70fb47fe97ad1d04ba86f\" groups=\"backup\" />",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\" />",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"cf1acfcdf4751e0554ffa765d03e479ec491cad6\" />",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"d6e17ad2b9a218e82569e09b761c226fa8df726a\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\" />",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"2d3ecc3de903a5e4d0bc9181adedb5e637f83435\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"8db06ae62940835d35db4de075bd68f0e00ea6b7\" />",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"62b6da664bb1d2da15fff20a500d55c559dae44a\" upstream=\"supported-newer-1.22\" dest-branch=\"supported-newer-1.22\" groups=\"kv\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"golang_protobuf_extensions\" path=\"godeps/src/github.com/matttproud/golang_protobuf_extensions\" remote=\"couchbasedeps\" revision=\"c182affec369e30f25d3eb8cd8a478dee585ae7d\" />",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"8171511423044401c5ebf4c5cad1bd1397835548\" groups=\"backup\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"5cab28d46a698a6fb1228df37e97f76c688f8134\" />",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"985a725631e25051d6d21c6ee35f1f6035df1ee6\" groups=\"bsl\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"c88803ceb88c652e01f48cf46cdf7e3dd6de065b\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" groups=\"bsl\" />",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\" />",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"c57616b187889a5318688f49817ccaceb9c098b9\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"a3b6bdf737c653bd589bc125968b5f2f16222c09\" groups=\"bsl\" />",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\" />",
 "  <project name=\"kv_engine\" revision=\"b80914ca72f53e4dac3755408f21cb9cf897d583\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" groups=\"kv,bsl\" />",
 "  <project name=\"libcouchbase\" revision=\"684931e59cd87e0c6292e8142c2b18897be5b10c\" />",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"5c6c11ed14e80c6cf805f3e43e7a362392f7eec6\" groups=\"notdefault,enterprise,kv_ee\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"2f7e511a565bc470b3d6ae73836660fc21c84aab\" groups=\"bsl\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"b70d849f0207f7cfe7ebf32b2db35b534929e041\" groups=\"bsl\" />",
 "  <project name=\"ns_server\" revision=\"6f0b50eba93a18be8d517f16286606fee917e0d7\" groups=\"bsl\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\" />",
 "  <project name=\"perks\" path=\"godeps/src/github.com/beorn7/perks\" remote=\"couchbasedeps\" revision=\"37c8de3658fcb183f997c4e13e8337516ab753e6\" upstream=\"refs/tags/v1.0.1\" dest-branch=\"refs/tags/v1.0.1\" />",
 "  <project name=\"phosphor\" revision=\"c0a034fe407eec4723f2e01db2d72762efdbc276\" groups=\"bsl,kv\" />",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\" />",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"bd8ecd46f0ef67859504722ddd6156293dbaafd7\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"platform\" revision=\"0e7a802da6c66ae81c113a7314251a2816ccc9af\" groups=\"bsl,kv\" />",
 "  <project name=\"procfs\" path=\"godeps/src/github.com/prometheus/procfs\" remote=\"couchbasedeps\" revision=\"76fc8b844e3a18c31bf689e4fe7efdd5a2f41298\" />",
 "  <project name=\"product-metadata\" revision=\"70a2fa8f353ae76274d15a8357e96b53c5575138\" groups=\"notdefault,packaging\" />",
 "  <project name=\"product-texts\" revision=\"a2495004ad0a30424e697b5eaf2aebb8b5d20231\" upstream=\"master\" dest-branch=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"d04d7b157bb510b1e0c10132224b616ac0e26b17\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\" />",
 "  <project name=\"protobuf-go\" path=\"godeps/src/google.golang.org/protobuf\" remote=\"couchbasedeps\" revision=\"32051b4f86e54c2142c7c05362c6e96ae3454a1c\" upstream=\"refs/tags/v1.28.0\" dest-branch=\"refs/tags/v1.28.0\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"edba4db729a8f6e070a0a3b355dc18aed3bfc959\" upstream=\"7.6.5\" dest-branch=\"7.6.5\" groups=\"bsl\" />",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"d553514237b4c396e1c2f0618f46154ddbef224a\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"query-ui\" revision=\"c5009a66c1c2e0f5ed8d868d4bb17ba337a5ca9d\" groups=\"bsl\" />",
 "  <project name=\"regulator\" path=\"goproj/src/github.com/couchbase/regulator\" remote=\"couchbase-priv\" revision=\"4ef404748ecc34fd87bdebc56074ebe99d240464\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"sigar\" revision=\"2da0c123cfb45ae39e76e730bd960db8812e3f20\" groups=\"kv\" />",
 "  <project name=\"simdutf\" path=\"third_party/simdutf\" remote=\"couchbasedeps\" revision=\"4a212616ba23c65c7048f9604faccbff5353300f\" upstream=\"refs/tags/v3.2.14\" dest-branch=\"refs/tags/v3.2.14\" groups=\"kv\" />",
 "  <project name=\"subjson\" revision=\"a619faccb30e43a4bc0708ee11b1b24abb349f18\" groups=\"bsl,kv\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\" />",
 "  <project name=\"testrunner\" revision=\"85bec5b61ca902b04401bd3e6fd6ca271dab9458\" upstream=\"trinity\" dest-branch=\"trinity\" />",
 "  <project name=\"tlm\" revision=\"fcefcae9a8a7e28736c9da7720d44f82a3a20dbc\" groups=\"bsl,kv\">",
 "    <copyfile src=\"Build.sh\" dest=\"Build.sh\" />",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\" />",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\" />",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\" />",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\" />",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\" />",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vbmap\" revision=\"6cce93c4af4497d8108c3ed31b84d7139321cc82\" />",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"d206985433728b2e0f848323d0d30e2a73f8d13f\" groups=\"notdefault,packaging\" />",
 "  <project name=\"xxhash\" path=\"goproj/src/github.com/cespare/xxhash\" remote=\"couchbasedeps\" revision=\"e7a6b52374f7e2abfb8abb27249d53a1997b09a7\" upstream=\"refs/tags/v2.1.2\" dest-branch=\"refs/tags/v2.1.2\" />",
 "</manifest>"]

[error_logger:info,2025-03-28T10:13:22.962Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.238.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:22.965Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.239.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:22.973Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2025-03-28T10:13:23.017Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.246.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.018Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.021Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.248.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.024Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.249.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.030Z,ns_1@cb.local:chronicle_agent_sup<0.250.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.251.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.031Z,ns_1@cb.local:chronicle_agent_sup<0.250.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.252.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2025-03-28T10:13:23.141Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_storage:open_logs:180]Reading log files [{0,
                    "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log"}]
[chronicle:info,2025-03-28T10:13:23.154Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_storage:open_current_log:232]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2025-03-28T10:13:23.155Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_storage:create_log:280]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2025-03-28T10:13:23.185Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2025-03-28T10:13:23.213Z,ns_1@cb.local:chronicle_agent_sup<0.250.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.253.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.214Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.250.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.223Z,ns_1@cb.local:chronicle_sup<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.255.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.224Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-03-28T10:13:23.229Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2025-03-28T10:13:23.229Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2025-03-28T10:13:23.235Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"c53116532f2f01b0da00786946759424">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"eca040df4ab6759679a51ead0aa2c8e2">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"c53116532f2f01b0da00786946759424">>,0}]}
[error_logger:info,2025-03-28T10:13:23.249Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.257.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.259Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.259.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.262Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.258.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.260.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.263Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.258.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.269Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.261.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2025-03-28T10:13:23.300Z,ns_1@cb.local:chronicle_config_rsm<0.265.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2025-03-28T10:13:23.334Z,ns_1@cb.local:chronicle_server<0.261.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.265.0>
[error_logger:info,2025-03-28T10:13:23.334Z,ns_1@cb.local:<0.264.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.264.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.265.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"eca040df4ab6759679a51ead0aa2c8e2">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.335Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,dynamic_supervisor}
    started: [{pid,<0.264.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"eca040df4ab6759679a51ead0aa2c8e2">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.341Z,ns_1@cb.local:<0.267.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.267.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.268.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2025-03-28T10:13:23.351Z,ns_1@cb.local:kv<0.269.0>:chronicle_rsm:get_incarnation:1594]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2025-03-28T10:13:23.376Z,ns_1@cb.local:chronicle_server<0.261.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.269.0>
[error_logger:info,2025-03-28T10:13:23.376Z,ns_1@cb.local:<0.267.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.267.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.269.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"eca040df4ab6759679a51ead0aa2c8e2">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.376Z,ns_1@cb.local:<0.263.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.263.0>,dynamic_supervisor}
    started: [{pid,<0.267.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"eca040df4ab6759679a51ead0aa2c8e2">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.376Z,ns_1@cb.local:<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.256.0>,dynamic_supervisor}
    started: [{pid,<0.262.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-03-28T10:13:23.379Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_upgrade:initialize:70]Setup initial chronicle content [{set,counters,[]},
                                 {set,auto_reprovision_cfg,
                                  [{enabled,true},{max_nodes,1},{count,0}]},
                                 {set,bucket_names,[]},
                                 {set,nodes_wanted,['ns_1@cb.local']},
                                 {set,server_groups,
                                  [[{uuid,<<"0">>},
                                    {name,<<"Group 1">>},
                                    {nodes,['ns_1@cb.local']}]]},
                                 {set,
                                  {node,'ns_1@cb.local',membership},
                                  active},
                                 {set,autocompaction,
                                  [{database_fragmentation_threshold,
                                    {30,undefined}},
                                   {view_fragmentation_threshold,
                                    {30,undefined}},
                                   {magma_fragmentation_percentage,50}]}]
[chronicle:debug,2025-03-28T10:13:23.566Z,ns_1@cb.local:chronicle_leader<0.257.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2025-03-28T10:13:23.566Z,ns_1@cb.local:<0.271.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"c53116532f2f01b0da00786946759424">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2025-03-28T10:13:23.567Z,ns_1@cb.local:<0.271.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2025-03-28T10:13:23.567Z,ns_1@cb.local:chronicle_leader<0.257.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"c53116532f2f01b0da00786946759424">>)
[chronicle:debug,2025-03-28T10:13:23.582Z,ns_1@cb.local:chronicle_agent<0.253.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"c53116532f2f01b0da00786946759424">>
[chronicle:debug,2025-03-28T10:13:23.584Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"c53116532f2f01b0da00786946759424">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"eca040df4ab6759679a51ead0aa2c8e2">>,
          <<"c53116532f2f01b0da00786946759424">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"c53116532f2f01b0da00786946759424">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"eca040df4ab6759679a51ead0aa2c8e2">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"c53116532f2f01b0da00786946759424">>,0}]}},
          {log_entry,<<"c53116532f2f01b0da00786946759424">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"eca040df4ab6759679a51ead0aa2c8e2">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"c53116532f2f01b0da00786946759424">>,0}]}},
          undefined}
[chronicle:debug,2025-03-28T10:13:23.586Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"c53116532f2f01b0da00786946759424">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2025-03-28T10:13:23.587Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"c53116532f2f01b0da00786946759424">>
[chronicle:debug,2025-03-28T10:13:23.595Z,ns_1@cb.local:chronicle_proposer<0.272.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"c53116532f2f01b0da00786946759424">> is ready. Committed seqno: 2
[chronicle:info,2025-03-28T10:13:23.595Z,ns_1@cb.local:chronicle_leader<0.257.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[ns_server:info,2025-03-28T10:13:23.625Z,ns_1@cb.local:chronicle_local<0.240.0>:chronicle_upgrade:initialize:72]Chronicle content was initialized. Rev = {<<"c53116532f2f01b0da00786946759424">>,
                                          3}.
[error_logger:info,2025-03-28T10:13:23.626Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.240.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:23.629Z,ns_1@cb.local:ns_cluster<0.274.0>:ns_cluster:handle_info:514]Chronicle state is: provisioned
[error_logger:info,2025-03-28T10:13:23.629Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.274.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:23.633Z,ns_1@cb.local:sigar<0.276.0>:sigar:spawn_sigar:134]Spawning sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 43 and log file "/opt/couchbase/var/lib/couchbase/logs/sigar_port.log"
[error_logger:info,2025-03-28T10:13:23.635Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.276.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:23.637Z,ns_1@cb.local:ns_config_sup<0.277.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2025-03-28T10:13:23.640Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.278.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.641Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.279.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.641Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.280.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:23.773Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1113]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2025-03-28T10:13:23.776Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1127]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2025-03-28T10:13:23.777Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1132]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2025-03-28T10:13:23.789Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1135]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2025-03-28T10:13:23.796Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:load_config:1157]Here's full dynamic config we loaded + static & default config:
[{resource_management,
  [{bucket,
    [{resident_ratio,
      [{enabled,false},{couchstore_minimum,1},{magma_minimum,0.2}]},
     {data_size,[{enabled,false},{couchstore_maximum,2},{magma_maximum,16}]}]},
   {index,[]},
   {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
   {disk_usage,[{enabled,false},{maximum,96}]},
   {collections_per_quota,[{enabled,false},{maximum,1}]}]},
 {{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   false]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   8092]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9999]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {health_monitor_refresh_interval,[]},
 {service_orchestrator_weight,
  [{kv,10000},
   {index,1000},
   {fts,1000},
   {cbas,1000},
   {n1ql,100},
   {eventing,100},
   {backup,10}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     cert_expires_soon,cert_expired,memory_threshold,history_size_warning,
     stuck_rebalance,memcached_connections]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1213},
 {fts_memory_quota,414},
 {memory_quota,1392},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {deployment_model,{memcached_config_mgr,get_config_profile,[]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,true},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {ssl_sasl_mechanisms,{memcached_config_mgr,sasl_mechanisms,[]}},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {tcp_user_timeout,tcp_user_timeout},
     {always_collect_trace_info,always_collect_trace_info},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {tcp_user_timeout,30},
   {always_collect_trace_info,true},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {prune_age,0},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<58,136,3,202,30,164,30,196,118,222,122,73>>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\"}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,[]},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   {7,6}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|
   <<"1fb1e16f0b22f628693700f0e67dfb4b">>]}]
[ns_server:debug,2025-03-28T10:13:23.805Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-03-28T10:13:23.805Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.281.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.808Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.284.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.808Z,ns_1@cb.local:ns_config_sup<0.277.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.285.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.808Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.277.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:23.813Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',n2n_client_cert_auth} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|false]
[ns_server:debug,2025-03-28T10:13:23.814Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]},
 {inet,false}]
[error_logger:info,2025-03-28T10:13:23.814Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.287.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:23.814Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|false]
[ns_server:debug,2025-03-28T10:13:23.814Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376003}}]}|inet]
[error_logger:info,2025-03-28T10:13:23.822Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.290.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.843Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.293.0>},
              {id,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.847Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.298.0>},
              {id,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:23.856Z,ns_1@cb.local:menelaus_barrier<0.299.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2025-03-28T10:13:23.856Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.299.0>},
              {id,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:23.867Z,ns_1@cb.local:<0.302.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    rest_lhttpc_pool} with delay of 1000
[error_logger:info,2025-03-28T10:13:23.867Z,ns_1@cb.local:<0.303.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.303.0>,suppress_max_restart_intensity}
    started: [{pid,<0.304.0>},
              {id,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.868Z,ns_1@cb.local:<0.301.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.301.0>,suppress_max_restart_intensity}
    started: [{pid,<0.302.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      rest_lhttpc_pool}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,rest_lhttpc_pool},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => rest_lhttpc_pool,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [lhttpc_manager],
                          restart => permanent,shutdown => 1000,
                          start =>
                              {lhttpc_manager,start_link,
                                  [[{name,rest_lhttpc_pool},
                                    {connection_timeout,120000},
                                    {pool_size,20}]]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.869Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.300.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.301.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,rest_lhttpc_pool}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => rest_lhttpc_pool,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [lhttpc_manager],
                         restart => permanent,shutdown => 1000,
                         start =>
                             {lhttpc_manager,start_link,
                                 [[{name,rest_lhttpc_pool},
                                   {connection_timeout,120000},
                                   {pool_size,20}]]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.869Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.300.0>},
              {id,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:23.879Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.305.0>},
              {id,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.884Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.306.0>},
              {id,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:23.890Z,ns_1@cb.local:ns_ssl_services_sup<0.307.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.308.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:23.978Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:868]Considering to store CA certs
[ns_server:debug,2025-03-28T10:13:24.436Z,ns_1@cb.local:<0.313.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-03-28T10:13:24.436Z,ns_1@cb.local:<0.313.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-03-28T10:13:24.436Z,ns_1@cb.local:<0.313.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-03-28T10:13:24.452Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_server_cert:generate_cert_and_pkey:156]Generated certificate and private key in 455404 us
[ns_server:debug,2025-03-28T10:13:24.578Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:883]Updating CA file with 1 certificates
[ns_server:info,2025-03-28T10:13:24.656Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:886]CA file updated: 1 cert(s) written
[ns_server:info,2025-03-28T10:13:24.658Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:should_regenerate_certs:935]Should regenerate node_cert because there are no certs on this node
[ns_server:debug,2025-03-28T10:13:25.186Z,ns_1@cb.local:<0.318.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-03-28T10:13:25.186Z,ns_1@cb.local:<0.318.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-03-28T10:13:25.186Z,ns_1@cb.local:<0.318.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-03-28T10:13:25.296Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs:1004]New node_cert and pkey are written to tmp file
[ns_server:info,2025-03-28T10:13:25.443Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs_phase2:1031]node_cert cert and pkey files updated
[ns_server:debug,2025-03-28T10:13:25.445Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376005}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,63981569604},
 {verified_with,<<7,141,87,8,72,154,50,82,162,88,191,181,38,21,127,8>>},
 {load_timestamp,63910376005},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGDDv4MQDvCQwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA4MzVjNGY4NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgODM1YzRm\nODUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC3i7Ju5C+jHgZVfF3C\nWJ9OYLrzWVoiQecXzeBVzY13ZGOdoj0WeiNi9Vjg1QcA95TcWPbFkev3hJ4lajo4\nG5S+JuF2WLz"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIGDDv4NKRgIkwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA4MzVjNGY4NTAeFw0yNTAzMjcxMDEzMjRaFw0y\nNzA2MzAxMDEzMjRaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAN1g3+3H\n9K14HGGAhGMH07/NQ6qhcAsgrdnzGAdhFANiBq43fqIPw2sKBwyXUErTRqG68OJX\nAzVBHP6"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {hostname,"127.0.0.1"}]
[ns_server:info,2025-03-28T10:13:25.523Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:should_regenerate_certs:935]Should regenerate client_cert because there are no certs on this node
[ns_server:debug,2025-03-28T10:13:25.995Z,ns_1@cb.local:<0.324.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:debug,2025-03-28T10:13:25.995Z,ns_1@cb.local:<0.324.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:info,2025-03-28T10:13:25.995Z,ns_1@cb.local:<0.324.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-03-28T10:13:26.136Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs:1004]New client_cert and pkey are written to tmp file
[ns_server:info,2025-03-28T10:13:26.250Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:save_certs_phase2:1031]client_cert cert and pkey files updated
[ns_server:debug,2025-03-28T10:13:26.251Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',client_cert} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376006}}]},
 {subject,<<"CN=Couchbase Internal Client (41880292)">>},
 {not_after,63981569605},
 {verified_with,<<7,141,87,8,72,154,50,82,162,88,191,181,38,21,127,8>>},
 {load_timestamp,63910376005},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIIGDDv4MQDvCQwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA4MzVjNGY4NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgODM1YzRm\nODUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQC3i7Ju5C+jHgZVfF3C\nWJ9OYLrzWVoiQecXzeBVzY13ZGOdoj0WeiNi9Vjg1QcA95TcWPbFkev3hJ4lajo4\nG5S+JuF2WLz"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDWTCCAkGgAwIBAgIIGDDv4QZsRUQwDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciA4MzVjNGY4NTAeFw0yNTAzMjcxMDEzMjVaFw0y\nNzA2MzAxMDEzMjVaMC8xLTArBgNVBAMTJENvdWNoYmFzZSBJbnRlcm5hbCBDbGll\nbnQgKDQxODgwMjkyKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAKzH\nhFjIz7HlGLRdaEH3jFijUBW4Jys/4e8TNHIk+kNZQ/5QeWoV60E31vYKrHTBiAO6\nPGEcgvg"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {type,generated},
 {name,"@internal"}]
[ns_server:debug,2025-03-28T10:13:26.357Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1495]Time left before client cert regeneration: 70588799000
[ns_server:info,2025-03-28T10:13:26.358Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:handle_info:800]cert_and_pkey changed
[error_logger:info,2025-03-28T10:13:26.358Z,ns_1@cb.local:ns_ssl_services_sup<0.307.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.309.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:26.440Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-03-28T10:13:26.442Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-03-28T10:13:26.442Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-03-28T10:13:26.443Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-03-28T10:13:26.444Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:debug,2025-03-28T10:13:26.470Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [capi_ssl_service,cb_dist_tls,
                                     client_cert_event,memcached,
                                     server_cert_event,ssl_service]
[ns_server:info,2025-03-28T10:13:26.471Z,ns_1@cb.local:<0.343.0>:ns_ssl_services_setup:notify_service:1227]Successfully notified service client_cert_event
[ns_server:debug,2025-03-28T10:13:26.471Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2025-03-28T10:13:26.471Z,ns_1@cb.local:<0.347.0>:ns_ssl_services_setup:notify_service:1227]Successfully notified service server_cert_event
[error_logger:info,2025-03-28T10:13:26.471Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.348.0>},
              {id,timer_server},
              {mfargs,{timer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:26.481Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Ensure config is going to change listeners. Will be stopped: [], will be started: [], will be restarted: []
[ns_server:info,2025-03-28T10:13:26.481Z,ns_1@cb.local:<0.345.0>:ns_ssl_services_setup:notify_service:1227]Successfully notified service cb_dist_tls
[error_logger:info,2025-03-28T10:13:26.485Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011363,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:26.485Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:26.486Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655937.12473>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:26.486Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655937.12473>,
                                  inet_tcp_dist,<0.349.0>,
                                  #Ref<0.3437536922.3327655939.10733>}
[error_logger:info,2025-03-28T10:13:26.486Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.349.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:26.487Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:26.487Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655937.12473>,
                               inet_tcp_dist,<0.349.0>,
                               #Ref<0.3437536922.3327655939.10733>}
[ns_server:debug,2025-03-28T10:13:26.487Z,ns_1@cb.local:<0.342.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1248}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1224}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-03-28T10:13:26.487Z,ns_1@cb.local:<0.342.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:warn,2025-03-28T10:13:26.490Z,ns_1@cb.local:<0.344.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-03-28T10:13:26.549Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,48,239,224,196,3,
              188,36,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,56,51,53,99,52,102,56,53,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,56,51,53,
              99,52,102,56,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,183,139,178,110,228,
              47,163,30,6,85,124,93,194,88,159,78,96,186,243,89,90,34,65,231,
              23,205,224,85,205,141,119,100,99,157,162,61,22,122,35,98,245,
              88,224,213,7,0,247,148,220,88,246,197,145,235,247,132,158,37,
              106,58,56,27,148,190,38,225,118,88,188,209,21,95,225,251,113,
              117,16,74,123,161,191,85,211,44,91,45,207,247,144,37,120,49,83,
              140,141,139,42,24,228,61,200,97,167,89,17,208,70,88,200,112,36,
              103,172,91,67,151,53,130,14,248,110,99,27,185,119,159,148,124,
              190,159,229,177,13,59,22,243,70,204,156,230,17,36,127,141,234,
              217,41,230,124,242,47,157,168,49,118,60,73,118,212,180,179,172,
              72,122,104,85,233,107,148,178,174,131,130,184,137,120,27,138,
              48,140,130,188,190,90,182,167,90,229,7,128,5,14,156,19,163,52,
              7,113,131,123,16,169,248,58,148,241,198,50,124,76,255,109,252,
              53,170,196,20,204,224,17,108,90,84,242,0,251,156,175,192,243,
              238,71,140,109,198,109,173,227,49,196,73,90,177,210,119,154,
              252,149,79,190,149,61,102,158,106,65,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,204,23,187,
              105,72,102,92,121,31,134,174,100,213,240,51,157,78,204,169,72,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,178,75,
              102,95,47,190,173,17,141,172,22,7,142,169,147,186,163,60,104,
              146,53,114,68,110,10,73,112,8,98,132,187,7,119,121,159,109,1,
              142,118,191,135,234,60,144,253,140,198,130,194,196,86,65,104,
              87,250,0,197,126,143,227,168,80,144,46,212,174,24,78,63,39,2,
              211,24,116,28,178,85,98,76,202,212,154,99,193,102,220,113,163,
              151,77,227,142,134,33,253,192,130,112,83,54,125,18,13,79,229,
              193,23,74,29,243,82,227,118,207,142,164,53,152,123,183,46,4,8,
              156,82,146,70,60,216,180,19,17,135,82,195,241,247,136,219,153,
              218,134,196,204,160,34,1,169,71,239,44,141,80,185,225,143,30,
              43,223,95,105,180,0,34,4,122,54,96,198,9,79,50,47,217,122,124,
              56,134,180,134,161,150,140,242,16,255,166,141,125,82,153,66,24,
              205,134,140,96,128,251,157,58,0,131,212,250,222,229,90,225,158,
              198,163,250,109,0,247,95,25,27,130,45,133,62,34,33,251,54,236,
              238,70,12,41,230,167,96,55,230,202,86,153,167,191,78,155,237,
              148,51,76,141,222,140,173,128,194,191,202>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-03-28T10:13:26.554Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.351.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.17878886>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:26.563Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-03-28T10:13:26.564Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-03-28T10:13:26.565Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-03-28T10:13:26.565Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-03-28T10:13:26.565Z,ns_1@cb.local:<0.330.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-03-28T10:13:26.574Z,ns_1@cb.local:<0.330.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,48,239,224,196,3,
              188,36,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,56,51,53,99,52,102,56,53,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,56,51,53,
              99,52,102,56,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,183,139,178,110,228,
              47,163,30,6,85,124,93,194,88,159,78,96,186,243,89,90,34,65,231,
              23,205,224,85,205,141,119,100,99,157,162,61,22,122,35,98,245,
              88,224,213,7,0,247,148,220,88,246,197,145,235,247,132,158,37,
              106,58,56,27,148,190,38,225,118,88,188,209,21,95,225,251,113,
              117,16,74,123,161,191,85,211,44,91,45,207,247,144,37,120,49,83,
              140,141,139,42,24,228,61,200,97,167,89,17,208,70,88,200,112,36,
              103,172,91,67,151,53,130,14,248,110,99,27,185,119,159,148,124,
              190,159,229,177,13,59,22,243,70,204,156,230,17,36,127,141,234,
              217,41,230,124,242,47,157,168,49,118,60,73,118,212,180,179,172,
              72,122,104,85,233,107,148,178,174,131,130,184,137,120,27,138,
              48,140,130,188,190,90,182,167,90,229,7,128,5,14,156,19,163,52,
              7,113,131,123,16,169,248,58,148,241,198,50,124,76,255,109,252,
              53,170,196,20,204,224,17,108,90,84,242,0,251,156,175,192,243,
              238,71,140,109,198,109,173,227,49,196,73,90,177,210,119,154,
              252,149,79,190,149,61,102,158,106,65,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,204,23,187,
              105,72,102,92,121,31,134,174,100,213,240,51,157,78,204,169,72,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,178,75,
              102,95,47,190,173,17,141,172,22,7,142,169,147,186,163,60,104,
              146,53,114,68,110,10,73,112,8,98,132,187,7,119,121,159,109,1,
              142,118,191,135,234,60,144,253,140,198,130,194,196,86,65,104,
              87,250,0,197,126,143,227,168,80,144,46,212,174,24,78,63,39,2,
              211,24,116,28,178,85,98,76,202,212,154,99,193,102,220,113,163,
              151,77,227,142,134,33,253,192,130,112,83,54,125,18,13,79,229,
              193,23,74,29,243,82,227,118,207,142,164,53,152,123,183,46,4,8,
              156,82,146,70,60,216,180,19,17,135,82,195,241,247,136,219,153,
              218,134,196,204,160,34,1,169,71,239,44,141,80,185,225,143,30,
              43,223,95,105,180,0,34,4,122,54,96,198,9,79,50,47,217,122,124,
              56,134,180,134,161,150,140,242,16,255,166,141,125,82,153,66,24,
              205,134,140,96,128,251,157,58,0,131,212,250,222,229,90,225,158,
              198,163,250,109,0,247,95,25,27,130,45,133,62,34,33,251,54,236,
              238,70,12,41,230,167,96,55,230,202,86,153,167,191,78,155,237,
              148,51,76,141,222,140,173,128,194,191,202>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-03-28T10:13:26.580Z,ns_1@cb.local:<0.330.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.330.0>,menelaus_web}
    started: [{pid,<0.370.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.17878886>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:26.581Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2025-03-28T10:13:26.581Z,ns_1@cb.local:ns_ssl_services_sup<0.307.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.328.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:26.582Z,ns_1@cb.local:<0.328.0>:restartable:loop:65]Restarting child <0.330.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
  Shutdown policy: 1000
  Caller: {<0.346.0>,#Ref<0.3437536922.3327655939.10806>}
[error_logger:info,2025-03-28T10:13:26.582Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.307.0>},
              {id,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:26.582Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.330.0>
[ns_server:info,2025-03-28T10:13:26.595Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-03-28T10:13:26.596Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-03-28T10:13:26.598Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-03-28T10:13:26.598Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-03-28T10:13:26.599Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-03-28T10:13:26.602Z,ns_1@cb.local:<0.389.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,48,239,224,196,3,
              188,36,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,56,51,53,99,52,102,56,53,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,56,51,53,
              99,52,102,56,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,183,139,178,110,228,
              47,163,30,6,85,124,93,194,88,159,78,96,186,243,89,90,34,65,231,
              23,205,224,85,205,141,119,100,99,157,162,61,22,122,35,98,245,
              88,224,213,7,0,247,148,220,88,246,197,145,235,247,132,158,37,
              106,58,56,27,148,190,38,225,118,88,188,209,21,95,225,251,113,
              117,16,74,123,161,191,85,211,44,91,45,207,247,144,37,120,49,83,
              140,141,139,42,24,228,61,200,97,167,89,17,208,70,88,200,112,36,
              103,172,91,67,151,53,130,14,248,110,99,27,185,119,159,148,124,
              190,159,229,177,13,59,22,243,70,204,156,230,17,36,127,141,234,
              217,41,230,124,242,47,157,168,49,118,60,73,118,212,180,179,172,
              72,122,104,85,233,107,148,178,174,131,130,184,137,120,27,138,
              48,140,130,188,190,90,182,167,90,229,7,128,5,14,156,19,163,52,
              7,113,131,123,16,169,248,58,148,241,198,50,124,76,255,109,252,
              53,170,196,20,204,224,17,108,90,84,242,0,251,156,175,192,243,
              238,71,140,109,198,109,173,227,49,196,73,90,177,210,119,154,
              252,149,79,190,149,61,102,158,106,65,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,204,23,187,
              105,72,102,92,121,31,134,174,100,213,240,51,157,78,204,169,72,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,178,75,
              102,95,47,190,173,17,141,172,22,7,142,169,147,186,163,60,104,
              146,53,114,68,110,10,73,112,8,98,132,187,7,119,121,159,109,1,
              142,118,191,135,234,60,144,253,140,198,130,194,196,86,65,104,
              87,250,0,197,126,143,227,168,80,144,46,212,174,24,78,63,39,2,
              211,24,116,28,178,85,98,76,202,212,154,99,193,102,220,113,163,
              151,77,227,142,134,33,253,192,130,112,83,54,125,18,13,79,229,
              193,23,74,29,243,82,227,118,207,142,164,53,152,123,183,46,4,8,
              156,82,146,70,60,216,180,19,17,135,82,195,241,247,136,219,153,
              218,134,196,204,160,34,1,169,71,239,44,141,80,185,225,143,30,
              43,223,95,105,180,0,34,4,122,54,96,198,9,79,50,47,217,122,124,
              56,134,180,134,161,150,140,242,16,255,166,141,125,82,153,66,24,
              205,134,140,96,128,251,157,58,0,131,212,250,222,229,90,225,158,
              198,163,250,109,0,247,95,25,27,130,45,133,62,34,33,251,54,236,
              238,70,12,41,230,167,96,55,230,202,86,153,167,191,78,155,237,
              148,51,76,141,222,140,173,128,194,191,202>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2025-03-28T10:13:26.607Z,ns_1@cb.local:<0.389.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.389.0>,menelaus_web}
    started: [{pid,<0.390.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.17878886>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:26.617Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-03-28T10:13:26.618Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-03-28T10:13:26.618Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-03-28T10:13:26.619Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-03-28T10:13:26.619Z,ns_1@cb.local:<0.389.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[error_logger:info,2025-03-28T10:13:26.621Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.410.0>},
              {id,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:26.622Z,ns_1@cb.local:<0.389.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,12,48,130,1,244,160,3,2,1,2,2,8,24,48,239,224,196,3,
              188,36,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,56,51,53,99,52,102,56,53,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,56,51,53,
              99,52,102,56,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,183,139,178,110,228,
              47,163,30,6,85,124,93,194,88,159,78,96,186,243,89,90,34,65,231,
              23,205,224,85,205,141,119,100,99,157,162,61,22,122,35,98,245,
              88,224,213,7,0,247,148,220,88,246,197,145,235,247,132,158,37,
              106,58,56,27,148,190,38,225,118,88,188,209,21,95,225,251,113,
              117,16,74,123,161,191,85,211,44,91,45,207,247,144,37,120,49,83,
              140,141,139,42,24,228,61,200,97,167,89,17,208,70,88,200,112,36,
              103,172,91,67,151,53,130,14,248,110,99,27,185,119,159,148,124,
              190,159,229,177,13,59,22,243,70,204,156,230,17,36,127,141,234,
              217,41,230,124,242,47,157,168,49,118,60,73,118,212,180,179,172,
              72,122,104,85,233,107,148,178,174,131,130,184,137,120,27,138,
              48,140,130,188,190,90,182,167,90,229,7,128,5,14,156,19,163,52,
              7,113,131,123,16,169,248,58,148,241,198,50,124,76,255,109,252,
              53,170,196,20,204,224,17,108,90,84,242,0,251,156,175,192,243,
              238,71,140,109,198,109,173,227,49,196,73,90,177,210,119,154,
              252,149,79,190,149,61,102,158,106,65,2,3,1,0,1,163,66,48,64,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,1,134,48,15,6,3,85,29,19,1,1,
              255,4,5,48,3,1,1,255,48,29,6,3,85,29,14,4,22,4,20,204,23,187,
              105,72,102,92,121,31,134,174,100,213,240,51,157,78,204,169,72,
              48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,3,130,1,1,0,178,75,
              102,95,47,190,173,17,141,172,22,7,142,169,147,186,163,60,104,
              146,53,114,68,110,10,73,112,8,98,132,187,7,119,121,159,109,1,
              142,118,191,135,234,60,144,253,140,198,130,194,196,86,65,104,
              87,250,0,197,126,143,227,168,80,144,46,212,174,24,78,63,39,2,
              211,24,116,28,178,85,98,76,202,212,154,99,193,102,220,113,163,
              151,77,227,142,134,33,253,192,130,112,83,54,125,18,13,79,229,
              193,23,74,29,243,82,227,118,207,142,164,53,152,123,183,46,4,8,
              156,82,146,70,60,216,180,19,17,135,82,195,241,247,136,219,153,
              218,134,196,204,160,34,1,169,71,239,44,141,80,185,225,143,30,
              43,223,95,105,180,0,34,4,122,54,96,198,9,79,50,47,217,122,124,
              56,134,180,134,161,150,140,242,16,255,166,141,125,82,153,66,24,
              205,134,140,96,128,251,157,58,0,131,212,250,222,229,90,225,158,
              198,163,250,109,0,247,95,25,27,130,45,133,62,34,33,251,54,236,
              238,70,12,41,230,167,96,55,230,202,86,153,167,191,78,155,237,
              148,51,76,141,222,140,173,128,194,191,202>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[#{cipher => aes_256_gcm,key_exchange => any,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm_8,key_exchange => any,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_ecdsa,
              mac => aead,prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => ecdhe_rsa,
              mac => aead,prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_ccm,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_128_ccm_8,key_exchange => ecdhe_ecdsa,mac => aead,
              prf => default_prf},
            #{cipher => aes_256_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => ecdh_ecdsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => ecdh_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_256_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_128_gcm,key_exchange => dhe_dss,mac => aead,
              prf => sha256},
            #{cipher => chacha20_poly1305,key_exchange => dhe_rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa_psk,mac => aead,
              prf => sha256},
            #{cipher => aes_256_gcm,key_exchange => rsa,mac => aead,
              prf => sha384},
            #{cipher => aes_128_gcm,key_exchange => rsa,mac => aead,
              prf => sha256},
            #{cipher => aes_256_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf},
            #{cipher => aes_128_cbc,key_exchange => rsa,mac => sha,
              prf => default_prf}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[ns_server:debug,2025-03-28T10:13:26.626Z,ns_1@cb.local:cb_saml<0.430.0>:cb_saml:handle_info:316]Settings have changed or this is the first start
[error_logger:info,2025-03-28T10:13:26.626Z,ns_1@cb.local:<0.389.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.389.0>,menelaus_web}
    started: [{pid,<0.409.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.11.17878886>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:26.627Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.430.0>},
              {id,cb_saml},
              {mfargs,{cb_saml,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:26.627Z,ns_1@cb.local:<0.328.0>:restartable:start_child:92]Started child process <0.389.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[ns_server:debug,2025-03-28T10:13:26.627Z,ns_1@cb.local:cb_saml<0.430.0>:cb_saml:restart_refresh_timer:586]Restarting refresh timer: 40158 ms
[ns_server:info,2025-03-28T10:13:26.627Z,ns_1@cb.local:<0.346.0>:ns_ssl_services_setup:notify_service:1227]Successfully notified service ssl_service
[ns_server:info,2025-03-28T10:13:26.627Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1207]Succesfully notified services [ssl_service,server_cert_event,
                               client_cert_event,cb_dist_tls]
[error_logger:info,2025-03-28T10:13:26.632Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.433.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:26.648Z,ns_1@cb.local:users_storage_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.435.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:26.652Z,ns_1@cb.local:users_replicator<0.435.0>:replicated_storage:wait_for_startup:47]Start waiting for startup
[ns_server:debug,2025-03-28T10:13:26.661Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:announce_startup:61]Announce my startup to <0.435.0>
[ns_server:debug,2025-03-28T10:13:26.661Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:open:177]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2025-03-28T10:13:26.661Z,ns_1@cb.local:users_replicator<0.435.0>:replicated_storage:wait_for_startup:50]Received replicated storage registration from <0.436.0>
[error_logger:info,2025-03-28T10:13:26.662Z,ns_1@cb.local:users_storage_sup<0.434.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.436.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:26.663Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.434.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-03-28T10:13:26.666Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-03-28T10:13:26.666Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1495]Time left before client cert regeneration: 70588799000
[ns_server:debug,2025-03-28T10:13:26.666Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:868]Considering to store CA certs
[ns_server:debug,2025-03-28T10:13:26.687Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2025-03-28T10:13:26.687Z,ns_1@cb.local:<0.445.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:26.687Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011364,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:26.688Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:26.688Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.10845>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:26.688Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.10845>,
                                  inet_tcp_dist,<0.446.0>,
                                  #Ref<0.3437536922.3327655939.10847>}
[ns_server:debug,2025-03-28T10:13:26.689Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.10845>,
                               inet_tcp_dist,<0.446.0>,
                               #Ref<0.3437536922.3327655939.10847>}
[error_logger:info,2025-03-28T10:13:26.689Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.446.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:26.689Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:26.689Z,ns_1@cb.local:<0.444.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1248}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1224}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-03-28T10:13:26.690Z,ns_1@cb.local:<0.444.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2025-03-28T10:13:26.694Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2025-03-28T10:13:26.695Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.448.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:26.709Z,ns_1@cb.local:users_sup<0.432.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.451.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:26.710Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.432.0>},
              {id,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:26.716Z,ns_1@cb.local:kernel_safe_sup<0.68.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.455.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:26.720Z,ns_1@cb.local:kernel_safe_sup<0.68.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.456.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:26.752Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-03-28T10:13:26.753Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1495]Time left before client cert regeneration: 70588799000
[ns_server:info,2025-03-28T10:13:26.794Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:validate_pkey:1068]Private key (node_cert) passphrase validation suceeded
[ns_server:info,2025-03-28T10:13:26.803Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:validate_pkey:1068]Private key (client_cert) passphrase validation suceeded
[ns_server:debug,2025-03-28T10:13:26.803Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2025-03-28T10:13:26.804Z,ns_1@cb.local:<0.463.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:26.804Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011365,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:26.804Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:26.804Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655937.12530>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:26.805Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655937.12530>,
                                  inet_tcp_dist,<0.465.0>,
                                  #Ref<0.3437536922.3327655937.12533>}
[ns_server:debug,2025-03-28T10:13:26.805Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655937.12530>,
                               inet_tcp_dist,<0.465.0>,
                               #Ref<0.3437536922.3327655937.12533>}
[error_logger:info,2025-03-28T10:13:26.805Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.465.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:26.805Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:26.806Z,ns_1@cb.local:<0.464.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1248}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1224}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-03-28T10:13:26.806Z,ns_1@cb.local:<0.464.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:debug,2025-03-28T10:13:26.806Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:select_from_table:312][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2025-03-28T10:13:26.806Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:init_after_ack:170]Loading 0 items, 305 words took 145ms
[ns_server:info,2025-03-28T10:13:26.835Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[ns_server:debug,2025-03-28T10:13:26.835Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376006}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2025-03-28T10:13:26.835Z,ns_1@cb.local:<0.469.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    start_couchdb_node} with delay of 5000
[ns_server:debug,2025-03-28T10:13:26.835Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376006}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[error_logger:info,2025-03-28T10:13:26.933Z,ns_1@cb.local:<0.470.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.470.0>,suppress_max_restart_intensity}
    started: [{pid,<0.472.0>},
              {id,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:26.934Z,ns_1@cb.local:<0.468.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.468.0>,suppress_max_restart_intensity}
    started: [{pid,<0.469.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      start_couchdb_node}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,start_couchdb_node},
                       5000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 5,id => start_couchdb_node,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => 86400000,
                          start => {ns_server_nodes_sup,start_couchdb_node,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:26.934Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.468.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,start_couchdb_node}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 5,id => start_couchdb_node,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => 86400000,
                         start => {ns_server_nodes_sup,start_couchdb_node,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:26.935Z,ns_1@cb.local:wait_link_to_couchdb_node<0.473.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:158]Waiting for ns_couchdb node to start
[error_logger:info,2025-03-28T10:13:26.935Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011366,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:26.936Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:26.936Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.10963>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:26.936Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.10963>,
                                  inet_tcp_dist,<0.475.0>,
                                  #Ref<0.3437536922.3327655937.12559>}
[ns_server:debug,2025-03-28T10:13:26.937Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.10963>,
                               inet_tcp_dist,<0.475.0>,
                               #Ref<0.3437536922.3327655937.12559>}
[error_logger:info,2025-03-28T10:13:26.937Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.475.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:26.937Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:26.937Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:27.138Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011367,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:27.138Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:27.139Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.10971>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:27.139Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.10971>,
                                  inet_tcp_dist,<0.477.0>,
                                  #Ref<0.3437536922.3327655940.10974>}
[ns_server:debug,2025-03-28T10:13:27.140Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.10971>,
                               inet_tcp_dist,<0.477.0>,
                               #Ref<0.3437536922.3327655940.10974>}
[error_logger:info,2025-03-28T10:13:27.140Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.477.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:27.140Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:27.140Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:27.341Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011368,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:27.341Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:27.342Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.10982>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:27.342Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.10982>,
                                  inet_tcp_dist,<0.479.0>,
                                  #Ref<0.3437536922.3327655940.10985>}
[error_logger:info,2025-03-28T10:13:27.347Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.479.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:27.347Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:27.347Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.10982>,
                               inet_tcp_dist,<0.479.0>,
                               #Ref<0.3437536922.3327655940.10985>}
[ns_server:debug,2025-03-28T10:13:27.347Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:27.549Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011369,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:27.549Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:27.550Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.10995>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:27.550Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.10995>,
                                  inet_tcp_dist,<0.481.0>,
                                  #Ref<0.3437536922.3327655940.10998>}
[ns_server:debug,2025-03-28T10:13:27.550Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.10995>,
                               inet_tcp_dist,<0.481.0>,
                               #Ref<0.3437536922.3327655940.10998>}
[error_logger:info,2025-03-28T10:13:27.550Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.481.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:27.551Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:27.551Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:27.751Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011370,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:27.752Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:27.752Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.10936>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:27.752Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.10936>,
                                  inet_tcp_dist,<0.483.0>,
                                  #Ref<0.3437536922.3327655940.11000>}
[ns_server:debug,2025-03-28T10:13:27.753Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.10936>,
                               inet_tcp_dist,<0.483.0>,
                               #Ref<0.3437536922.3327655940.11000>}
[error_logger:info,2025-03-28T10:13:27.753Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.483.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:27.753Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:27.753Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:27.954Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011371,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:27.954Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:27.954Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.10948>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:27.955Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.10948>,
                                  inet_tcp_dist,<0.485.0>,
                                  #Ref<0.3437536922.3327655939.10951>}
[error_logger:info,2025-03-28T10:13:27.955Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.485.0>,shutdown}}
[error_logger:info,2025-03-28T10:13:27.956Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:27.956Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.10948>,
                               inet_tcp_dist,<0.485.0>,
                               #Ref<0.3437536922.3327655939.10951>}
[ns_server:debug,2025-03-28T10:13:27.956Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:28.157Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011372,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:28.157Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:28.158Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.10961>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:28.158Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.10961>,
                                  inet_tcp_dist,<0.487.0>,
                                  #Ref<0.3437536922.3327655939.10964>}
[ns_server:debug,2025-03-28T10:13:28.338Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.10961>,
                               inet_tcp_dist,<0.487.0>,
                               #Ref<0.3437536922.3327655939.10964>}
[error_logger:info,2025-03-28T10:13:28.338Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.487.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:28.339Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:28.339Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:28.540Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011373,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:28.541Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:28.541Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11010>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:28.541Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11010>,
                                  inet_tcp_dist,<0.489.0>,
                                  #Ref<0.3437536922.3327655940.11013>}
[ns_server:info,2025-03-28T10:13:28.573Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: =ERROR REPORT==== 28-Mar-2025::10:13:28.338458 ===
ns_couchdb<0.472.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.472.0>: 

[ns_server:debug,2025-03-28T10:13:28.621Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11010>,
                               inet_tcp_dist,<0.489.0>,
                               #Ref<0.3437536922.3327655940.11013>}
[error_logger:info,2025-03-28T10:13:28.621Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.489.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:28.621Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:28.621Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2025-03-28T10:13:28.822Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: =ERROR REPORT==== 28-Mar-2025::10:13:28.620994 ===
ns_couchdb<0.472.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.472.0>: 

[error_logger:info,2025-03-28T10:13:28.823Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011374,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:28.823Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:28.823Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.10983>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:28.823Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.10983>,
                                  inet_tcp_dist,<0.491.0>,
                                  #Ref<0.3437536922.3327655939.10986>}
[error_logger:info,2025-03-28T10:13:28.919Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.491.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-03-28T10:13:28.919Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.10983>,
                               inet_tcp_dist,<0.491.0>,
                               #Ref<0.3437536922.3327655939.10986>}
[error_logger:info,2025-03-28T10:13:28.919Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:28.919Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2025-03-28T10:13:29.120Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: =ERROR REPORT==== 28-Mar-2025::10:13:28.919090 ===
ns_couchdb<0.472.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.472.0>: 

[error_logger:info,2025-03-28T10:13:29.120Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011375,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:29.121Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:29.121Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11476>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:29.121Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11476>,
                                  inet_tcp_dist,<0.493.0>,
                                  #Ref<0.3437536922.3327655940.11027>}
[ns_server:debug,2025-03-28T10:13:29.175Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11476>,
                               inet_tcp_dist,<0.493.0>,
                               #Ref<0.3437536922.3327655940.11027>}
[error_logger:info,2025-03-28T10:13:29.175Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.493.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:29.175Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:29.175Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2025-03-28T10:13:29.375Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: =ERROR REPORT==== 28-Mar-2025::10:13:29.172531 ===
ns_couchdb<0.472.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.472.0>: 

[error_logger:info,2025-03-28T10:13:29.376Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011376,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:29.377Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:29.377Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655937.12580>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:29.377Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655937.12580>,
                                  inet_tcp_dist,<0.495.0>,
                                  #Ref<0.3437536922.3327655937.12583>}
[ns_server:debug,2025-03-28T10:13:29.457Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655937.12580>,
                               inet_tcp_dist,<0.495.0>,
                               #Ref<0.3437536922.3327655937.12583>}
[error_logger:info,2025-03-28T10:13:29.457Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.495.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:29.457Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:29.457Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2025-03-28T10:13:29.664Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: =ERROR REPORT==== 28-Mar-2025::10:13:29.456841 ===
ns_couchdb<0.472.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.472.0>: 

[error_logger:info,2025-03-28T10:13:29.664Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011377,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:29.664Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:29.664Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11039>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:29.665Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11039>,
                                  inet_tcp_dist,<0.497.0>,
                                  #Ref<0.3437536922.3327655940.11042>}
[error_logger:info,2025-03-28T10:13:29.715Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.497.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:29.715Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:29.715Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11039>,
                               inet_tcp_dist,<0.497.0>,
                               #Ref<0.3437536922.3327655940.11042>}
[ns_server:debug,2025-03-28T10:13:29.715Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:29.916Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011378,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:29.917Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:29.917Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11053>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:29.917Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11053>,
                                  inet_tcp_dist,<0.499.0>,
                                  #Ref<0.3437536922.3327655940.11056>}
[error_logger:info,2025-03-28T10:13:29.951Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.499.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-03-28T10:13:29.951Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11053>,
                               inet_tcp_dist,<0.499.0>,
                               #Ref<0.3437536922.3327655940.11056>}
[error_logger:info,2025-03-28T10:13:29.952Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:29.952Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:30.152Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011379,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:30.152Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:30.152Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.11000>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:30.153Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.11000>,
                                  inet_tcp_dist,<0.501.0>,
                                  #Ref<0.3437536922.3327655939.11003>}
[ns_server:debug,2025-03-28T10:13:30.154Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.11000>,
                               inet_tcp_dist,<0.501.0>,
                               #Ref<0.3437536922.3327655939.11003>}
[error_logger:info,2025-03-28T10:13:30.154Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.501.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:30.154Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:30.154Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:30.355Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011380,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:30.356Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:30.356Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.11014>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:30.356Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.11014>,
                                  inet_tcp_dist,<0.503.0>,
                                  #Ref<0.3437536922.3327655939.11017>}
[ns_server:debug,2025-03-28T10:13:30.364Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.11014>,
                               inet_tcp_dist,<0.503.0>,
                               #Ref<0.3437536922.3327655939.11017>}
[error_logger:info,2025-03-28T10:13:30.364Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.503.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:30.364Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:30.364Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:30.565Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011381,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:30.566Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:30.566Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11065>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:30.566Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11065>,
                                  inet_tcp_dist,<0.505.0>,
                                  #Ref<0.3437536922.3327655940.11068>}
[error_logger:info,2025-03-28T10:13:30.576Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.505.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:30.577Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:30.577Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-03-28T10:13:30.577Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11065>,
                               inet_tcp_dist,<0.505.0>,
                               #Ref<0.3437536922.3327655940.11068>}
[error_logger:info,2025-03-28T10:13:30.777Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011382,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:30.777Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:30.778Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11508>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:30.778Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11508>,
                                  inet_tcp_dist,<0.507.0>,
                                  #Ref<0.3437536922.3327655938.11511>}
[error_logger:info,2025-03-28T10:13:30.780Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.507.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-03-28T10:13:30.780Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11508>,
                               inet_tcp_dist,<0.507.0>,
                               #Ref<0.3437536922.3327655938.11511>}
[error_logger:info,2025-03-28T10:13:30.780Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:30.780Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:30.981Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011383,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:30.981Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:30.982Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11516>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:30.982Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11516>,
                                  inet_tcp_dist,<0.509.0>,
                                  #Ref<0.3437536922.3327655938.11519>}
[ns_server:debug,2025-03-28T10:13:30.988Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11516>,
                               inet_tcp_dist,<0.509.0>,
                               #Ref<0.3437536922.3327655938.11519>}
[error_logger:info,2025-03-28T10:13:30.988Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.509.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:30.989Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:30.989Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:31.190Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011384,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.191Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.191Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11524>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.192Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11524>,
                                  inet_tcp_dist,<0.511.0>,
                                  #Ref<0.3437536922.3327655938.11527>}
[ns_server:debug,2025-03-28T10:13:31.194Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11524>,
                               inet_tcp_dist,<0.511.0>,
                               #Ref<0.3437536922.3327655938.11527>}
[error_logger:info,2025-03-28T10:13:31.194Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.511.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.194Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.194Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:31.395Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011385,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.395Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.395Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11539>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.396Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11539>,
                                  inet_tcp_dist,<0.513.0>,
                                  #Ref<0.3437536922.3327655938.11542>}
[ns_server:debug,2025-03-28T10:13:31.398Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11539>,
                               inet_tcp_dist,<0.513.0>,
                               #Ref<0.3437536922.3327655938.11542>}
[error_logger:info,2025-03-28T10:13:31.398Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.513.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.398Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.398Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:31.599Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011386,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.600Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.600Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655937.12596>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.600Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655937.12596>,
                                  inet_tcp_dist,<0.515.0>,
                                  #Ref<0.3437536922.3327655937.12599>}
[ns_server:debug,2025-03-28T10:13:31.601Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655937.12596>,
                               inet_tcp_dist,<0.515.0>,
                               #Ref<0.3437536922.3327655937.12599>}
[error_logger:info,2025-03-28T10:13:31.601Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.515.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.601Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.602Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-03-28T10:13:31.667Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2025-03-28T10:13:31.668Z,ns_1@cb.local:<0.522.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:31.668Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011387,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.668Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.668Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11555>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.668Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11555>,
                                  inet_tcp_dist,<0.523.0>,
                                  #Ref<0.3437536922.3327655938.11558>}
[ns_server:debug,2025-03-28T10:13:31.670Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11555>,
                               inet_tcp_dist,<0.523.0>,
                               #Ref<0.3437536922.3327655938.11558>}
[error_logger:info,2025-03-28T10:13:31.670Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.523.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.670Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.670Z,ns_1@cb.local:<0.521.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1248}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1224}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-03-28T10:13:31.670Z,ns_1@cb.local:<0.521.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-03-28T10:13:31.701Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:debug,2025-03-28T10:13:31.753Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2025-03-28T10:13:31.753Z,ns_1@cb.local:<0.529.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:31.753Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011388,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.754Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.754Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11582>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.754Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11582>,
                                  inet_tcp_dist,<0.531.0>,
                                  #Ref<0.3437536922.3327655938.11585>}
[ns_server:debug,2025-03-28T10:13:31.756Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11582>,
                               inet_tcp_dist,<0.531.0>,
                               #Ref<0.3437536922.3327655938.11585>}
[error_logger:info,2025-03-28T10:13:31.756Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.531.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.756Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.756Z,ns_1@cb.local:<0.530.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1248}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1224}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-03-28T10:13:31.756Z,ns_1@cb.local:<0.530.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-03-28T10:13:31.779Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service},
                                                      {{error,no_proccess},
                                                       memcached}]
[error_logger:info,2025-03-28T10:13:31.802Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011389,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.803Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.803Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11093>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.803Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11093>,
                                  inet_tcp_dist,<0.533.0>,
                                  #Ref<0.3437536922.3327655940.11096>}
[ns_server:debug,2025-03-28T10:13:31.805Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11093>,
                               inet_tcp_dist,<0.533.0>,
                               #Ref<0.3437536922.3327655940.11096>}
[error_logger:info,2025-03-28T10:13:31.805Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.533.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.805Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.805Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:debug,2025-03-28T10:13:31.836Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [capi_ssl_service,memcached]
[ns_server:warn,2025-03-28T10:13:31.836Z,ns_1@cb.local:<0.540.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:31.837Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011390,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:31.837Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:31.837Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11109>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:31.837Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11109>,
                                  inet_tcp_dist,<0.541.0>,
                                  #Ref<0.3437536922.3327655939.11056>}
[ns_server:debug,2025-03-28T10:13:31.839Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11109>,
                               inet_tcp_dist,<0.541.0>,
                               #Ref<0.3437536922.3327655939.11056>}
[error_logger:info,2025-03-28T10:13:31.839Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.541.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:31.839Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:31.839Z,ns_1@cb.local:<0.539.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1248}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1224}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,199}]}]
[ns_server:warn,2025-03-28T10:13:31.840Z,ns_1@cb.local:<0.539.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2025-03-28T10:13:31.863Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[error_logger:info,2025-03-28T10:13:32.007Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011391,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:32.007Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:32.007Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11116>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:32.008Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11116>,
                                  inet_tcp_dist,<0.543.0>,
                                  #Ref<0.3437536922.3327655940.11119>}
[ns_server:debug,2025-03-28T10:13:32.016Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11116>,
                               inet_tcp_dist,<0.543.0>,
                               #Ref<0.3437536922.3327655940.11119>}
[error_logger:info,2025-03-28T10:13:32.017Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.543.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:32.017Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:32.017Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:32.218Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011392,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:32.218Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:32.218Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11127>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:32.219Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11127>,
                                  inet_tcp_dist,<0.545.0>,
                                  #Ref<0.3437536922.3327655940.11130>}
[ns_server:debug,2025-03-28T10:13:32.220Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11127>,
                               inet_tcp_dist,<0.545.0>,
                               #Ref<0.3437536922.3327655940.11130>}
[error_logger:info,2025-03-28T10:13:32.220Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.545.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:32.221Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:32.221Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:32.421Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011393,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:32.422Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:32.422Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655939.11073>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:32.422Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655939.11073>,
                                  inet_tcp_dist,<0.547.0>,
                                  #Ref<0.3437536922.3327655939.11076>}
[ns_server:debug,2025-03-28T10:13:32.424Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655939.11073>,
                               inet_tcp_dist,<0.547.0>,
                               #Ref<0.3437536922.3327655939.11076>}
[error_logger:info,2025-03-28T10:13:32.424Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.547.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2025-03-28T10:13:32.425Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:32.425Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[error_logger:info,2025-03-28T10:13:32.626Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011394,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:32.627Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:32.627Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655940.11141>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:32.627Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655940.11141>,
                                  inet_tcp_dist,<0.549.0>,
                                  #Ref<0.3437536922.3327655938.11609>}
[ns_server:debug,2025-03-28T10:13:32.629Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655940.11141>,
                               inet_tcp_dist,<0.549.0>,
                               #Ref<0.3437536922.3327655938.11609>}
[error_logger:info,2025-03-28T10:13:32.629Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.549.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:32.629Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:32.629Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:32.830Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011395,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:32.830Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:32.831Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11621>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:32.831Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11621>,
                                  inet_tcp_dist,<0.551.0>,
                                  #Ref<0.3437536922.3327655937.12653>}
[ns_server:debug,2025-03-28T10:13:32.833Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11621>,
                               inet_tcp_dist,<0.551.0>,
                               #Ref<0.3437536922.3327655937.12653>}
[error_logger:info,2025-03-28T10:13:32.833Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.551.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2025-03-28T10:13:32.833Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2025-03-28T10:13:32.834Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2025-03-28T10:13:33.034Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011396,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:13:33.034Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:13:33.035Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.11634>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:13:33.035Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.11634>,
                                  inet_tcp_dist,<0.553.0>,
                                  #Ref<0.3437536922.3327655938.11636>}
[ns_server:debug,2025-03-28T10:13:33.041Z,ns_1@cb.local:<0.474.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:172]ns_couchdb is not ready: false
[ns_server:info,2025-03-28T10:13:33.843Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: Apache CouchDB v4.5.1-334-ga7cd1de8 (LogLevel=info) is starting.

[ns_server:info,2025-03-28T10:13:34.121Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2025-03-28T10:13:34.517Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.473.0>},
              {id,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.120652322>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:34.534Z,ns_1@cb.local:<0.559.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_disksup} with delay of 4000
[error_logger:info,2025-03-28T10:13:34.547Z,ns_1@cb.local:<0.560.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.560.0>,suppress_max_restart_intensity}
    started: [{pid,<0.561.0>},
              {id,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:34.548Z,ns_1@cb.local:<0.558.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.558.0>,suppress_max_restart_intensity}
    started: [{pid,<0.559.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_disksup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_disksup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_disksup,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:34.548Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.558.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_disksup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_disksup,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_disksup,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:34.565Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.562.0>},
              {id,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:34.581Z,ns_1@cb.local:ns_server_sup<0.557.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2025-03-28T10:13:34.582Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.563.0>},
              {id,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:34.588Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.564.0>},
              {id,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:34.596Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.565.0>},
              {id,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-03-28T10:13:34.608Z,ns_1@cb.local:ns_log<0.567.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-03-28T10:13:34.608Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.567.0>},
              {id,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:34.609Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.568.0>},
              {id,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2025-03-28T10:13:34.615Z,ns_1@cb.local:event_log_server<0.569.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2025-03-28T10:13:34.615Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.569.0>},
              {id,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:34.718Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: 234: Booted. Waiting for shutdown request
ns_couchdb<0.472.0>: 234: Booted. Waiting for shutdown request
ns_couchdb<0.472.0>: working as port

[error_logger:info,2025-03-28T10:13:34.825Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.571.0>},
              {id,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:34.829Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.573.0>},
              {id,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:34.829Z,ns_1@cb.local:<0.575.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    ns_babysitter_log_consumer} with delay of 4000
[error_logger:info,2025-03-28T10:13:34.830Z,ns_1@cb.local:<0.576.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.576.0>,suppress_max_restart_intensity}
    started: [{pid,<0.577.0>},
              {id,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:34.830Z,ns_1@cb.local:<0.574.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.574.0>,suppress_max_restart_intensity}
    started: [{pid,<0.575.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,
                           ns_babysitter_log_consumer},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_babysitter_log_consumer,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start =>
                              {ns_log,start_link_babysitter_log_consumer,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:34.831Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.574.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,
                      ns_babysitter_log_consumer}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_babysitter_log_consumer,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start =>
                             {ns_log,start_link_babysitter_log_consumer,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:35.168Z,ns_1@cb.local:<0.582.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-03-28T10:13:35.168Z,ns_1@cb.local:<0.582.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-03-28T10:13:35.168Z,ns_1@cb.local:<0.582.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:debug,2025-03-28T10:13:35.297Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2025-03-28T10:13:35.596Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:ensure_prometheus_config:932]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2025-03-28T10:13:35.700Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376015}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"A1N6PFle1eKogNULdUFY6EJpMa5RcYS66j4M0F1+i6o=">>}},
    {<<"sha512">>,
     {[{<<"s">>,
        <<"m9J/gpn714JPXBzfgoUGucJDi7EvIdX5OFGc9pdJ1rYPmP+21f2PSNozDa1IktI4X8RXso10SQ0SgE0kzzkgXA==">>},
       {<<"h">>,
        {sanitized,<<"P4s+lY6wOZK1lOoBhSoy7iOHmxma9SCOq+YiA3Uji60=">>}},
       {<<"i">>,15000}]}},
    {<<"sha256">>,
     {[{<<"s">>,<<"UTUBnkmvr2IybGExoVygGnUIzyq7JaUk1ac5ggtlqsQ=">>},
       {<<"h">>,
        {sanitized,<<"Wd356POhflWxzXywUhTLWkG55N12xeLawZ91/bF27/U=">>}},
       {<<"i">>,15000}]}},
    {<<"sha1">>,
     {[{<<"s">>,<<"p6DonXfQupSLBlKmJt+SpNYET2Y=">>},
       {<<"h">>,
        {sanitized,<<"TWK6ZK1k1DJYW+cWrR3HOLATA1JeouVMBk9Bvyukjkc=">>}},
       {<<"i">>,15000}]}}]}}]
[ns_server:debug,2025-03-28T10:13:35.773Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:apply_config:724]Restarting Prometheus as the start specs have changed
[error_logger:info,2025-03-28T10:13:35.800Z,ns_1@cb.local:ale_dynamic_sup<0.79.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.585.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.422Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.578.0>},
              {id,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.437Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:init:60]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-03-28T10:13:36.446Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-03-28T10:13:36.537Z,ns_1@cb.local:memcached_passwords<0.591.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-03-28T10:13:36.540Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-03-28T10:13:36.554Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-03-28T10:13:36.554Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-03-28T10:13:36.554Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:137]Refresh of [isasl] skipped. Retry in 1000 ms.
[error_logger:info,2025-03-28T10:13:36.555Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.591.0>},
              {id,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.567Z,ns_1@cb.local:memcached_permissions<0.603.0>:memcached_cfg:init:60]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-03-28T10:13:36.580Z,ns_1@cb.local:memcached_permissions<0.603.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-03-28T10:13:36.584Z,ns_1@cb.local:memcached_permissions<0.603.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-03-28T10:13:36.588Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-03-28T10:13:36.592Z,ns_1@cb.local:memcached_permissions<0.603.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-03-28T10:13:36.592Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[error_logger:info,2025-03-28T10:13:36.592Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.603.0>},
              {id,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.596Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.606.0>},
              {id,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.601Z,ns_1@cb.local:ns_node_disco_sup<0.608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.609.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.601Z,ns_1@cb.local:ns_node_disco<0.610.0>:ns_node_disco:init:111]Initting ns_node_disco with []
[ns_server:debug,2025-03-28T10:13:36.601Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2025-03-28T10:13:36.601Z,ns_1@cb.local:ns_node_disco_sup<0.608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.610.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.602Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
otp ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376016}}]},
 {cookie,{sanitized,<<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}}]
[user:info,2025-03-28T10:13:36.602Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:36.602Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-03-28T10:13:36.603Z,ns_1@cb.local:<0.612.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:36.603Z,ns_1@cb.local:<0.614.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[error_logger:info,2025-03-28T10:13:36.621Z,ns_1@cb.local:ns_node_disco_sup<0.608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.631.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.627Z,ns_1@cb.local:<0.614.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:36.627Z,ns_1@cb.local:<0.612.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[error_logger:info,2025-03-28T10:13:36.662Z,ns_1@cb.local:ns_config_rep_sup<0.651.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.656.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.662Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2025-03-28T10:13:36.663Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2025-03-28T10:13:36.665Z,ns_1@cb.local:ns_config_rep_sup<0.651.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.657.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.666Z,ns_1@cb.local:ns_node_disco_sup<0.608.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.651.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.666Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.608.0>},
              {id,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:36.673Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[error_logger:info,2025-03-28T10:13:36.674Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.667.0>},
              {id,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.679Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.669.0>},
              {id,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.682Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.673.0>},
              {id,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.689Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.675.0>},
              {id,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.690Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.678.0>},
              {id,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.690Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.679.0>},
              {id,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.690Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.680.0>},
              {id,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.695Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.681.0>},
              {id,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.704Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2025-03-28T10:13:36.704Z,ns_1@cb.local:<0.687.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:36.707Z,ns_1@cb.local:ns_heart_sup<0.682.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.689.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.708Z,ns_1@cb.local:ns_heart_sup<0.682.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.691.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.708Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.682.0>},
              {id,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.726Z,ns_1@cb.local:ns_doctor_sup<0.696.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.697.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:36.736Z,ns_1@cb.local:<0.688.0>:ns_ssl_services_setup:notify_service:1227]Successfully notified service capi_ssl_service
[ns_server:info,2025-03-28T10:13:36.736Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1207]Succesfully notified services [capi_ssl_service]
[error_logger:info,2025-03-28T10:13:36.745Z,ns_1@cb.local:ns_doctor_sup<0.696.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.698.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.745Z,ns_1@cb.local:<0.692.0>:restartable:start_child:92]Started child process <0.696.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2025-03-28T10:13:36.746Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.692.0>},
              {id,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.746Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.701.0>},
              {id,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.755Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.704.0>},
              {id,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.755Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.705.0>},
              {id,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.755Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.706.0>},
              {id,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.756Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.707.0>},
              {id,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.757Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.709.0>},
              {id,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.757Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.711.0>},
              {id,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.757Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.713.0>},
              {id,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.758Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.715.0>},
              {id,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.758Z,ns_1@cb.local:<0.717.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    encryption_service} with delay of 1000
[ns_server:debug,2025-03-28T10:13:36.759Z,ns_1@cb.local:encryption_service<0.719.0>:encryption_service:recover:199]Config marker /opt/couchbase/var/lib/couchbase/config/sm_load_config_marker doesn't exist
[error_logger:info,2025-03-28T10:13:36.760Z,ns_1@cb.local:<0.718.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.718.0>,suppress_max_restart_intensity}
    started: [{pid,<0.719.0>},
              {id,encryption_service},
              {mfargs,{encryption_service,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.760Z,ns_1@cb.local:<0.716.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.716.0>,suppress_max_restart_intensity}
    started: [{pid,<0.717.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      encryption_service}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,encryption_service},
                       1000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 1,id => encryption_service,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [encryption_service],
                          restart => permanent,shutdown => 5000,
                          start => {encryption_service,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.761Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.716.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,encryption_service}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 1,id => encryption_service,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [encryption_service],
                         restart => permanent,shutdown => 5000,
                         start => {encryption_service,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.775Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.722.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.775Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.724.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:36.783Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached}]
[ns_server:debug,2025-03-28T10:13:36.785Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [memcached]
[ns_server:warn,2025-03-28T10:13:36.785Z,ns_1@cb.local:<0.728.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2025-03-28T10:13:36.791Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.729.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.839Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.730.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:36.840Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached}]
[error_logger:info,2025-03-28T10:13:36.859Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.731.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.859Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.732.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.863Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [memcached]
[ns_server:warn,2025-03-28T10:13:36.863Z,ns_1@cb.local:<0.736.0>:ns_ssl_services_setup:notify_service:1229]Failed to notify service memcached: {error,no_proccess}
[ns_server:info,2025-03-28T10:13:36.908Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1218]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached}]
[error_logger:info,2025-03-28T10:13:36.912Z,ns_1@cb.local:inet_gethost_native_sup<0.740.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.741.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2025-03-28T10:13:36.912Z,ns_1@cb.local:menelaus_web_sup<0.737.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.739.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.912Z,ns_1@cb.local:kernel_safe_sup<0.68.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.740.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:36.913Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-03-28T10:13:36.913Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-03-28T10:13:36.914Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-03-28T10:13:36.914Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-03-28T10:13:36.915Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-03-28T10:13:36.916Z,ns_1@cb.local:<0.743.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2025-03-28T10:13:36.917Z,ns_1@cb.local:<0.743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.743.0>,menelaus_web}
    started: [{pid,<0.744.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:36.918Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2025-03-28T10:13:36.919Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2025-03-28T10:13:36.921Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2025-03-28T10:13:36.921Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2025-03-28T10:13:36.922Z,ns_1@cb.local:<0.743.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:126]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2025-03-28T10:13:36.925Z,ns_1@cb.local:<0.743.0>:menelaus_web:maybe_start_http_server:130]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2025-03-28T10:13:36.927Z,ns_1@cb.local:<0.743.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.743.0>,menelaus_web}
    started: [{pid,<0.761.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:36.927Z,ns_1@cb.local:<0.742.0>:restartable:start_child:92]Started child process <0.743.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2025-03-28T10:13:36.928Z,ns_1@cb.local:menelaus_web_sup<0.737.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.742.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2025-03-28T10:13:36.929Z,ns_1@cb.local:menelaus_sup<0.721.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.6.5-5704-enterprise".
[error_logger:info,2025-03-28T10:13:36.930Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.737.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.930Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.778.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.940Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.780.0>},
              {id,guardrail_monitor},
              {mfargs,{guardrail_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.948Z,ns_1@cb.local:menelaus_sup<0.721.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.781.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.948Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.721.0>},
              {id,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:36.948Z,ns_1@cb.local:<0.788.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_ports_setup} with delay of 4000
[error_logger:info,2025-03-28T10:13:36.949Z,ns_1@cb.local:<0.789.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.789.0>,suppress_max_restart_intensity}
    started: [{pid,<0.790.0>},
              {id,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.949Z,ns_1@cb.local:<0.787.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.787.0>,suppress_max_restart_intensity}
    started: [{pid,<0.788.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_ports_setup}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_ports_setup},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_ports_setup,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,
                          shutdown => brutal_kill,
                          start => {ns_ports_setup,start,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.949Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.787.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_ports_setup}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_ports_setup,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,
                         shutdown => brutal_kill,
                         start => {ns_ports_setup,start,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:36.951Z,ns_1@cb.local:ns_ports_setup<0.790.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2025-03-28T10:13:36.964Z,ns_1@cb.local:service_agent_sup<0.793.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.794.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.965Z,ns_1@cb.local:service_agent_sup<0.793.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.795.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.125430937>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:36.965Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.793.0>},
              {id,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:36.978Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.798.0>},
              {id,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.026Z,ns_1@cb.local:memcached_auth_server<0.799.0>:memcached_auth_server:reconnect:241]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2025-03-28T10:13:37.027Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.799.0>},
              {id,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.027Z,ns_1@cb.local:<0.803.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit_cfg} with delay of 4000
[ns_server:debug,2025-03-28T10:13:37.028Z,ns_1@cb.local:ns_audit_cfg<0.805.0>:ns_audit_cfg:write_audit_json:238]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "33985209"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {prune_age,
                                                                                      0},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:debug,2025-03-28T10:13:37.057Z,ns_1@cb.local:ns_audit_cfg<0.805.0>:ns_audit_cfg:notify_memcached:152]Instruct memcached to reload audit config
[error_logger:info,2025-03-28T10:13:37.057Z,ns_1@cb.local:<0.804.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.804.0>,suppress_max_restart_intensity}
    started: [{pid,<0.805.0>},
              {id,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.057Z,ns_1@cb.local:<0.802.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.802.0>,suppress_max_restart_intensity}
    started: [{pid,<0.803.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit_cfg}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit_cfg},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit_cfg,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.057Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.802.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit_cfg}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit_cfg,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit_cfg,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.058Z,ns_1@cb.local:<0.810.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,ns_audit} with delay of 4000
[ns_server:warn,2025-03-28T10:13:37.058Z,ns_1@cb.local:<0.808.0>:ns_memcached:connect:1463]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-03-28T10:13:37.074Z,ns_1@cb.local:<0.811.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.811.0>,suppress_max_restart_intensity}
    started: [{pid,<0.812.0>},
              {id,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.075Z,ns_1@cb.local:<0.809.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.809.0>,suppress_max_restart_intensity}
    started: [{pid,<0.810.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      ns_audit}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,ns_audit},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                          inherited_max_t => 10,modules => [],
                          restart => permanent,shutdown => 1000,
                          start => {ns_audit,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.075Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.809.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,ns_audit}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => ns_audit,inherited_max_r => 20,
                         inherited_max_t => 10,modules => [],
                         restart => permanent,shutdown => 1000,
                         start => {ns_audit,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.076Z,ns_1@cb.local:<0.815.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    memcached_config_mgr} with delay of 4000
[ns_server:debug,2025-03-28T10:13:37.076Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:memcached_port_pid:155]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-03-28T10:13:37.076Z,ns_1@cb.local:<0.816.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.816.0>,suppress_max_restart_intensity}
    started: [{pid,<0.817.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.076Z,ns_1@cb.local:<0.814.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.814.0>,suppress_max_restart_intensity}
    started: [{pid,<0.815.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      memcached_config_mgr}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,memcached_config_mgr},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => memcached_config_mgr,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [],restart => permanent,shutdown => 1000,
                          start => {memcached_config_mgr,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.077Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.814.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,memcached_config_mgr}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => memcached_config_mgr,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [],restart => permanent,shutdown => 1000,
                         start => {memcached_config_mgr,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2025-03-28T10:13:37.087Z,ns_1@cb.local:<0.818.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2025-03-28T10:13:37.088Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.818.0>},
              {id,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.098Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.819.0>},
              {id,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.108Z,ns_1@cb.local:<0.822.0>:memcached_config_mgr:memcached_port_pid:155]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-03-28T10:13:37.108Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.822.0>},
              {id,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.121Z,ns_1@cb.local:ns_bucket_worker_sup<0.826.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.827.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.127Z,ns_1@cb.local:ns_bucket_worker_sup<0.826.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.828.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.127Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.826.0>},
              {id,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:warn,2025-03-28T10:13:37.128Z,ns_1@cb.local:<0.831.0>:ns_memcached:connect:1463]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-03-28T10:13:37.128Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.832.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.141Z,ns_1@cb.local:<0.834.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.832.0>} exited with reason {noproc,
                                                                                {gen_statem,
                                                                                 call,
                                                                                 [mb_master,
                                                                                  master_node,
                                                                                  infinity]}}
[error_logger:info,2025-03-28T10:13:37.163Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.837.0>},
              {id,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.163Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.839.0>},
              {id,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.164Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.841.0>},
              {id,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.164Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.843.0>},
              {id,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.172Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.845.0>},
              {id,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.176Z,ns_1@cb.local:ns_heart<0.689.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-03-28T10:13:37.176Z,ns_1@cb.local:goxdcr_status_keeper<0.845.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2025-03-28T10:13:37.177Z,ns_1@cb.local:goxdcr_status_keeper<0.845.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-03-28T10:13:37.179Z,ns_1@cb.local:services_stats_sup<0.848.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.849.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.183Z,ns_1@cb.local:ns_heart<0.689.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:40]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2025-03-28T10:13:37.187Z,ns_1@cb.local:service_status_keeper_sup<0.850.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.851.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.204Z,ns_1@cb.local:service_status_keeper_sup<0.850.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.852.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.208Z,ns_1@cb.local:service_status_keeper_sup<0.850.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.856.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.214Z,ns_1@cb.local:service_status_keeper_sup<0.850.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.863.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.214Z,ns_1@cb.local:services_stats_sup<0.848.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.850.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.223Z,ns_1@cb.local:services_stats_sup<0.848.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.866.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,[#Fun<services_stats_sup.0.35188242>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.223Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.848.0>},
              {id,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.223Z,ns_1@cb.local:<0.869.0>:supervisor_cushion:init:39]Starting supervisor cushion for {suppress_max_restart_intensity,
                                    inherited_max_r_max_t_sup,
                                    compaction_daemon} with delay of 4000
[ns_server:debug,2025-03-28T10:13:37.244Z,ns_1@cb.local:<0.881.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.881.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2025-03-28T10:13:37.249Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:13:37.249Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:13:37.249Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[error_logger:info,2025-03-28T10:13:37.249Z,ns_1@cb.local:<0.870.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.870.0>,suppress_max_restart_intensity}
    started: [{pid,<0.879.0>},
              {id,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,86400000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.249Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:13:37.250Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:13:37.250Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2025-03-28T10:13:37.250Z,ns_1@cb.local:<0.868.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.868.0>,suppress_max_restart_intensity}
    started: [{pid,<0.869.0>},
              {id,{suppress_max_restart_intensity,supervisor_cushion,
                      compaction_daemon}},
              {mfargs,
                  {supervisor_cushion,start_link,
                      [{suppress_max_restart_intensity,
                           inherited_max_r_max_t_sup,compaction_daemon},
                       4000,infinity,suppress_max_restart_intensity,
                       inherited_max_r_max_t_sup_link,
                       [#{delay => 4,id => compaction_daemon,
                          inherited_max_r => 20,inherited_max_t => 10,
                          modules => [compaction_daemon],
                          restart => permanent,shutdown => 86400000,
                          start => {compaction_daemon,start_link,[]},
                          type => worker}],
                       #{always_delay => true}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.252Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.868.0>},
              {id,{suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup,compaction_daemon}},
              {mfargs,
                  {suppress_max_restart_intensity,
                      avoid_max_restart_intensity_sup_link,
                      [#{delay => 4,id => compaction_daemon,
                         inherited_max_r => 20,inherited_max_t => 10,
                         modules => [compaction_daemon],
                         restart => permanent,shutdown => 86400000,
                         start => {compaction_daemon,start_link,[]},
                         type => worker}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.258Z,ns_1@cb.local:cluster_logs_sup<0.887.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.888.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.258Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.887.0>},
              {id,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.259Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.890.0>},
              {id,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.259Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.892.0>},
              {id,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.259Z,ns_1@cb.local:ns_heart_slow_status_updater<0.691.0>:goxdcr_rest:get_from_goxdcr:139]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2025-03-28T10:13:37.274Z,ns_1@cb.local:leader_leases_sup<0.895.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.896.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.280Z,ns_1@cb.local:leader_leases_sup<0.895.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.898.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.280Z,ns_1@cb.local:leader_services_sup<0.894.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.895.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.289Z,ns_1@cb.local:leader_registry_sup<0.903.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.904.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.290Z,ns_1@cb.local:leader_registry_sup<0.903.0>:mb_master:check_master_takeover_needed:183]Sending master node question to the following nodes: []
[ns_server:debug,2025-03-28T10:13:37.291Z,ns_1@cb.local:leader_registry_sup<0.903.0>:mb_master:check_master_takeover_needed:187]Got replies: []
[ns_server:debug,2025-03-28T10:13:37.291Z,ns_1@cb.local:leader_registry_sup<0.903.0>:mb_master:check_master_takeover_needed:193]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2025-03-28T10:13:37.291Z,ns_1@cb.local:mb_master<0.906.0>:mb_master:init:86]Heartbeat interval is 2000
[user:info,2025-03-28T10:13:37.291Z,ns_1@cb.local:mb_master<0.906.0>:mb_master:init:91]I'm the only node, so I'm the master.
[ns_server:debug,2025-03-28T10:13:37.291Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2025-03-28T10:13:37.346Z,ns_1@cb.local:mb_master<0.906.0>:master_activity_events:submit_cast:75]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[error_logger:info,2025-03-28T10:13:37.355Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.909.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.363Z,ns_1@cb.local:leader_quorum_nodes_manager<0.912.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2025-03-28T10:13:37.363Z,ns_1@cb.local:leader_quorum_nodes_manager<0.912.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[error_logger:info,2025-03-28T10:13:37.363Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.912.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.367Z,ns_1@cb.local:leader_lease_agent<0.898.0>:leader_lease_agent:do_handle_acquire_lease:147]Granting lease to {lease_holder,<<"c33e2fe123e7f3070ea3e5d98a698a27">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:info,2025-03-28T10:13:37.372Z,ns_1@cb.local:mb_master_sup<0.908.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.918.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.372Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.918.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.380Z,ns_1@cb.local:<0.919.0>:chronicle_master:do_init:115]Starting with SelfRef = #Ref<0.3437536922.3327655940.11526>
[ns_server:info,2025-03-28T10:13:37.380Z,ns_1@cb.local:mb_master_sup<0.908.0>:misc:start_singleton:913]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.919.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.381Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.919.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.387Z,ns_1@cb.local:ns_orchestrator_sup<0.921.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.922.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:37.395Z,ns_1@cb.local:compat_mode_manager<0.923.0>:cluster_compat_mode:do_upgrades:205]Initiating rbac upgrade due to version change from [7,1] to [7,6] (target version: [7,
                                                                                    6])
[ns_server:info,2025-03-28T10:13:37.395Z,ns_1@cb.local:compat_mode_manager<0.923.0>:menelaus_users:upgrade:1057]Upgrading users database to [7,6]
[ns_server:debug,2025-03-28T10:13:37.396Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376017}}]}|
 started]
[ns_server:debug,2025-03-28T10:13:37.396Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([rbac_upgrade]..)
[ns_server:debug,2025-03-28T10:13:37.397Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 7 us
[ns_server:debug,2025-03-28T10:13:37.397Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-03-28T10:13:37.397Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 7 us
[ns_server:debug,2025-03-28T10:13:37.397Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-03-28T10:13:37.398Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-03-28T10:13:37.398Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:146]Received sync_token from {<0.939.0>,
                          [alias|#Ref<0.3437536922.3327721475.11340>]}
[ns_server:debug,2025-03-28T10:13:37.398Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:95]Received sync_token from {<0.939.0>,
                          [alias|#Ref<0.3437536922.3327721475.11340>]}
[ns_server:debug,2025-03-28T10:13:37.398Z,ns_1@cb.local:<0.936.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:debug,2025-03-28T10:13:37.398Z,ns_1@cb.local:users_storage<0.436.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,'_','_','_'},[],['$_']}],
                               100}
[ns_server:info,2025-03-28T10:13:37.398Z,ns_1@cb.local:compat_mode_manager<0.923.0>:menelaus_users:upgrade:1071]Users database was upgraded to [7,6]
[ns_server:debug,2025-03-28T10:13:37.398Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:151]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-03-28T10:13:37.399Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:100]Received sync_to_me with timeout = 60000, nodes = ['ns_1@cb.local']
[ns_server:debug,2025-03-28T10:13:37.399Z,ns_1@cb.local:users_storage<0.436.0>:replicated_storage:handle_call:146]Received sync_token from {<0.943.0>,
                          [alias|#Ref<0.3437536922.3327721475.11359>]}
[ns_server:debug,2025-03-28T10:13:37.399Z,ns_1@cb.local:users_replicator<0.435.0>:doc_replicator:loop:95]Received sync_token from {<0.943.0>,
                          [alias|#Ref<0.3437536922.3327721475.11359>]}
[ns_server:debug,2025-03-28T10:13:37.399Z,ns_1@cb.local:<0.940.0>:replicated_storage:handle_call:157]sync_to_me reply: ok
[ns_server:info,2025-03-28T10:13:37.399Z,ns_1@cb.local:compat_mode_manager<0.923.0>:menelaus_users:upgrade:1073]Users database upgrade was delivered to ['ns_1@cb.local']
[ns_server:info,2025-03-28T10:13:37.399Z,ns_1@cb.local:kv<0.269.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,1]. Final version = [7,6]
[ns_server:info,2025-03-28T10:13:37.399Z,ns_1@cb.local:kv<0.269.0>:chronicle_upgrade:upgrade_loop:114]Upgrading chronicle from [7,2]. Final version = [7,6]
[ns_server:info,2025-03-28T10:13:37.417Z,ns_1@cb.local:<0.911.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:306]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"c33e2fe123e7f3070ea3e5d98a698a27">>)
[ns_server:debug,2025-03-28T10:13:37.438Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-03-28T10:13:37.438Z,ns_1@cb.local:roles_cache<0.451.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-03-28T10:13:37.438Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {[7,
                                                                              6],
                                                                             {0,
                                                                              4075124975},
                                                                             {0,
                                                                              4075124975},
                                                                             false,
                                                                             []}
[ns_server:debug,2025-03-28T10:13:37.438Z,ns_1@cb.local:chronicle_kv_log<0.565.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"c53116532f2f01b0da00786946759424">>,
                                           6})
[7,6]
[ns_server:debug,2025-03-28T10:13:37.439Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-03-28T10:13:37.439Z,ns_1@cb.local:memcached_permissions<0.603.0>:memcached_permissions:producer:512]Skipping update during users upgrade
[ns_server:debug,2025-03-28T10:13:37.439Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:868]Considering to store CA certs
[ns_server:debug,2025-03-28T10:13:37.439Z,ns_1@cb.local:<0.945.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:37.440Z,ns_1@cb.local:<0.945.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:37.442Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2025-03-28T10:13:37.445Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]}]

[ns_server:info,2025-03-28T10:13:37.445Z,ns_1@cb.local:ns_config<0.281.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,2]
[ns_server:info,2025-03-28T10:13:37.446Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:debug,2025-03-28T10:13:37.496Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1191]Going to notify following services: [memcached]
[ns_server:info,2025-03-28T10:13:37.497Z,ns_1@cb.local:<0.952.0>:ns_ssl_services_setup:notify_service:1227]Successfully notified service memcached
[ns_server:info,2025-03-28T10:13:37.497Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:notify_services:1207]Succesfully notified services [memcached]
[ns_server:info,2025-03-28T10:13:37.505Z,ns_1@cb.local:ns_config<0.281.0>:ns_online_config_upgrader:do_upgrade_config:59]Performing online config upgrade to [7,6]
[ns_server:debug,2025-03-28T10:13:37.525Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1495]Time left before client cert regeneration: 70588788000
[ns_server:info,2025-03-28T10:13:37.551Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:do_upgrade_config:733]Upgrading config by changes:
[{set,cluster_compat_version,[7,6]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {16384,
        [{name,<<"remote cluster ref creation">>},
         {description,<<"created remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16385,
        [{name,<<"remote cluster ref update">>},
         {description,<<"updated remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16386,
        [{name,<<"remote cluster ref deletion">>},
         {description,<<"deleted remote cluster ref">>},
         {enabled,true},
         {module,xdcr}]},
       {16387,
        [{name,<<"replication creation">>},
         {description,<<"created replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16388,
        [{name,<<"replication pause">>},
         {description,<<"paused replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16389,
        [{name,<<"replication resume">>},
         {description,<<"resumed replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16390,
        [{name,<<"replication cancellation">>},
         {description,<<"canceled replication">>},
         {enabled,true},
         {module,xdcr}]},
       {16391,
        [{name,<<"default replication settings update">>},
         {description,<<"updated default replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16392,
        [{name,<<"individual replication settings update">>},
         {description,<<"updated individual replication settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16393,
        [{name,<<"bucket settings update">>},
         {description,<<"updated bucket settings">>},
         {enabled,true},
         {module,xdcr}]},
       {16394,
        [{name,<<"authorization failure while adding remote cluster ref">>},
         {description,<<"failed to add remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16395,
        [{name,<<"authorization failure while updating remote cluster ref">>},
         {description,<<"failed to update remote cluster ref because of authorization failure">>},
         {enabled,true},
         {module,xdcr}]},
       {16396,
        [{name,<<"access denied">>},
         {description,<<"access denied">>},
         {enabled,true},
         {module,xdcr}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {24576,
        [{name,<<"Delete index">>},
         {description,<<"FTS index was deleted">>},
         {enabled,true},
         {module,fts}]},
       {24577,
        [{name,<<"Create/Update index">>},
         {description,<<"FTS index was created/Updated">>},
         {enabled,true},
         {module,fts}]},
       {24579,
        [{name,<<"Control index">>},
         {description,<<"FTS index control command was issued">>},
         {enabled,true},
         {module,fts}]},
       {24582,
        [{name,<<"GC run">>},
         {description,<<"GC run was triggered">>},
         {enabled,true},
         {module,fts}]},
       {24583,
        [{name,<<"CPU profile">>},
         {description,<<"CPU profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {24584,
        [{name,<<"Memory profile">>},
         {description,<<"Memory profiling was started">>},
         {enabled,true},
         {module,fts}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initiate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {28730,
        [{name,<<"/admin/gc API request">>},
         {description,<<"An HTTP request was made to run garbage collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28731,
        [{name,<<"/admin/ffdc API request">>},
         {description,<<"An HTTP request was made to run an FFDC collection">>},
         {enabled,false},
         {module,n1ql}]},
       {28732,
        [{name,<<"/admin/log/ API request">>},
         {description,<<"An HTTP request was made to access diagnostic logs">>},
         {enabled,false},
         {module,n1ql}]},
       {28733,
        [{name,<<"/admin/sequences_cache API request">>},
         {description,<<"An HTTP request was made to access sequences">>},
         {enabled,false},
         {module,n1ql}]},
       {28734,
        [{name,<<"CREATE SEQUENCE statement">>},
         {description,<<"A N1QL CREATE SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28735,
        [{name,<<"ALTER SEQUENCE statement">>},
         {description,<<"A N1QL ALTER SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28736,
        [{name,<<"DROP SEQUENCE statement">>},
         {description,<<"A N1QL DROP SEQUENCE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28737,
        [{name,<<"Migration abort">>},
         {description,<<"Migration was aborted">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get real_userid eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{disable_max_count,false},
       {enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]},
 {delete,memory_alert_email},
 {delete,memory_alert_popup},
 {delete,popup_alerts_auto_failover_upgrade_70_fixed},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_shard_affinity\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.log_level\":\"info\",\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.rebalance.blob_storage_bucket\":\"\",\"indexer.settings.rebalance.blob_storage_prefix\":\"\",\"indexer.settings.rebalance.blob_storage_region\":\"\",\"indexer.settings.rebalance.blob_storage_scheme\":\"\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.thresholds.mem_high\":70,\"indexer.settings.thresholds.mem_low\":50,\"indexer.settings.thresholds.units_high\":60,\"indexer.settings.thresholds.units_low\":40}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"query.settings.tmp_space_size\":5120,\"scan-cap\":512,\"timeout\":0,\"txtimeout\":\"0ms\",\"use-cbo\":true,\"use-replica\":\"unset\"}">>},
 {set,{metakv,<<"/analytics/settings/config">>},
      <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>},
 {delete,mb33750_workaround_enabled},
 {delete,cert_and_pkey},
 {set,resource_management,
      [{bucket,[{resident_ratio,[{enabled,false},
                                 {couchstore_minimum,1},
                                 {magma_minimum,0.2}]},
                {data_size,[{enabled,false},
                            {couchstore_maximum,2},
                            {magma_maximum,16}]}]},
       {index,[]},
       {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
       {disk_usage,[{enabled,false},{maximum,96}]},
       {collections_per_quota,[{enabled,false},{maximum,1}]}]}]

[ns_server:debug,2025-03-28T10:13:37.573Z,ns_1@cb.local:tombstone_keeper<0.278.0>:tombstone_keeper:handle_call:49]Refreshed with timestamps []
[ns_server:debug,2025-03-28T10:13:37.573Z,ns_1@cb.local:ns_cookie_manager<0.239.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2025-03-28T10:13:37.574Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,rbac_upgrade,
                               resource_management,
                               {metakv,<<"/analytics/settings/config">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2025-03-28T10:13:37.576Z,ns_1@cb.local:<0.955.0>:ns_node_disco:do_nodes_wanted_updated_fun:210]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:37.577Z,ns_1@cb.local:<0.955.0>:ns_node_disco:do_nodes_wanted_updated_fun:213]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"yC94oAzo60ZLzUdidmsnEbLtErnzq/wwf5xK8WAOJog=">>}
[ns_server:debug,2025-03-28T10:13:37.580Z,ns_1@cb.local:roles_cache<0.451.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2025-03-28T10:13:37.582Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 5371 us
[ns_server:debug,2025-03-28T10:13:37.584Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:maybe_store_ca_certs:868]Considering to store CA certs
[ns_server:debug,2025-03-28T10:13:37.585Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2025-03-28T10:13:37.590Z,ns_1@cb.local:memcached_permissions<0.603.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-03-28T10:13:37.594Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
audit_decriptors ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376017}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {16384,
  [{name,<<"remote cluster ref creation">>},
   {description,<<"created remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16385,
  [{name,<<"remote cluster ref update">>},
   {description,<<"updated remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16386,
  [{name,<<"remote cluster ref deletion">>},
   {description,<<"deleted remote cluster ref">>},
   {enabled,true},
   {module,xdcr}]},
 {16387,
  [{name,<<"replication creation">>},
   {description,<<"created replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16388,
  [{name,<<"replication pause">>},
   {description,<<"paused replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16389,
  [{name,<<"replication resume">>},
   {description,<<"resumed replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16390,
  [{name,<<"replication cancellation">>},
   {description,<<"canceled replication">>},
   {enabled,true},
   {module,xdcr}]},
 {16391,
  [{name,<<"default replication settings update">>},
   {description,<<"updated default replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16392,
  [{name,<<"individual replication settings update">>},
   {description,<<"updated individual replication settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16393,
  [{name,<<"bucket settings update">>},
   {description,<<"updated bucket settings">>},
   {enabled,true},
   {module,xdcr}]},
 {16394,
  [{name,<<"authorization failure while adding remote cluster ref">>},
   {description,<<"failed to add remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16395,
  [{name,<<"authorization failure while updating remote cluster ref">>},
   {description,<<"failed to update remote cluster ref because of authorization failure">>},
   {enabled,true},
   {module,xdcr}]},
 {16396,
  [{name,<<"access denied">>},
   {description,<<"access denied">>},
   {enabled,true},
   {module,xdcr}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {24576,
  [{name,<<"Delete index">>},
   {description,<<"FTS index was deleted">>},
   {enabled,true},
   {module,fts}]},
 {24577,
  [{name,<<"Create/Update index">>},
   {description,<<"FTS index was created/Updated">>},
   {enabled,true},
   {module,fts}]},
 {24579,
  [{name,<<"Control index">>},
   {description,<<"FTS index control command was issued">>},
   {enabled,true},
   {module,fts}]},
 {24582,
  [{name,<<"GC run">>},
   {description,<<"GC run was triggered">>},
   {enabled,true},
   {module,fts}]},
 {24583,
  [{name,<<"CPU profile">>},
   {description,<<"CPU profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {24584,
  [{name,<<"Memory profile">>},
   {description,<<"Memory profiling was started">>},
   {enabled,true},
   {module,fts}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was execu"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statemen"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT st"...>>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO S"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statem"...>>},
   {description,<<"A N1QL SET TRANSACTION ISOLA"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT stateme"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API "...>>},
   {description,<<"An HTTP request was "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/trans"...>>},
   {description,<<"An HTTP request "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / re"...>>},
   {description,<<"An HTTP requ"...>>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutd"...>>},
   {description,<<"An HTTP "...>>},
   {enabled,false},
   {module,n1ql}]},
 {28730,
  [{name,<<"/admin/g"...>>},
   {description,<<"An H"...>>},
   {enabled,false},
   {module,...}]},
 {28731,[{name,<<"/adm"...>>},{description,<<...>>},{enabled,...},{...}]},
 {28732,[{name,<<...>>},{description,...},{...}|...]},
 {28733,[{name,...},{...}|...]},
 {28734,[{...}|...]},
 {28735,[...]},
 {28736,...},
 {...}|...]
[ns_server:debug,2025-03-28T10:13:37.587Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:handle_cast:168]Synchronized with merger in 2279 us
[user:warn,2025-03-28T10:13:37.596Z,ns_1@cb.local:compat_mode_manager<0.923.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from [7,1] to [7,6]
[error_logger:info,2025-03-28T10:13:37.596Z,ns_1@cb.local:ns_orchestrator_sup<0.921.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.923.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.596Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
cluster_compat_version ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{3,63910376017}}]},7,6]
[ns_server:debug,2025-03-28T10:13:37.596Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
rbac_upgrade ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{2,63910376017}}]}|
 '_deleted']
[ns_server:debug,2025-03-28T10:13:37.596Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{2,63910376017}}]},
 {disable_max_count,false},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2025-03-28T10:13:37.597Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
resource_management ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376017}}]},
 {bucket,[{resident_ratio,[{enabled,false},
                           {couchstore_minimum,1},
                           {magma_minimum,0.2}]},
          {data_size,[{enabled,false},
                      {couchstore_maximum,2},
                      {magma_maximum,16}]}]},
 {index,[]},
 {cores_per_bucket,[{enabled,false},{minimum,0.4}]},
 {disk_usage,[{enabled,false},{maximum,96}]},
 {collections_per_quota,[{enabled,false},{maximum,1}]}]
[ns_server:debug,2025-03-28T10:13:37.597Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/analytics/settings/config">>} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376017}}]}|
 <<"{\"analytics.settings.blob_storage_bucket\":\"\",\"analytics.settings.blob_storage_prefix\":\"\",\"analytics.settings.blob_storage_region\":\"\",\"analytics.settings.blob_storage_scheme\":\"\",\"analytics.settings.num_replicas\":0}">>]
[ns_server:debug,2025-03-28T10:13:37.597Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376017}}]}|
 <<"{\"indexer.settings.compaction.abort_exceed_interval\":false,\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.enable_"...>>]
[ns_server:debug,2025-03-28T10:13:37.597Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376017}}]}|
 <<"{\"cleanupclientattempts\":true,\"cleanuplostattempts\":true,\"cleanupwindow\":\"60s\",\"completed-limit\":4000,\"completed-max-plan-size\":262144,\"completed-threshold\":1000,\"loglevel\":\"info\",\"max-parallelism\":1,\"memory-quota\":0,\"n1ql-feat-ctrl\":76,\"node-quota\":0,\"node-quota-val-percent\":67,\"num-cpus\":0,\"numatrs\":1024,\"pipeline-batch\":16,\"pipeline-cap\":512,\"prepared-limit\":16384,\"query.settings.cu"...>>]
[ns_server:debug,2025-03-28T10:13:37.610Z,ns_1@cb.local:ns_ssl_services_setup<0.309.0>:ns_ssl_services_setup:restart_regenerate_client_cert_timer:1495]Time left before client cert regeneration: 70588788000
[ns_server:debug,2025-03-28T10:13:37.611Z,ns_1@cb.local:memcached_permissions<0.603.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-03-28T10:13:37.614Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2025-03-28T10:13:37.615Z,ns_1@cb.local:memcached_passwords<0.591.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2025-03-28T10:13:37.623Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{2,63910376017}}]}|
 {"@prometheus",
  {auth,
   [{<<"hash">>,
     {[{<<"hashes">>,
        {sanitized,<<"CCWv5cGxZxmveefRKeVnLzejR/VW1F3yxRbxl3vlQBw=">>}},
       {<<"algorithm">>,<<"argon2id">>},
       {<<"salt">>,<<"UNLQzMMsVCHHPkjJyKMxXQ==">>},
       {<<"parallelism">>,1},
       {<<"time">>,3},
       {<<"memory">>,524288}]}},
    {<<"scram-sha-512">>,
     {[{<<"salt">>,
        <<"R/UuSMt0s6+3vGc/1zqB8jK0FmYuB0/K4Lvo8mObf4COWa7boEbYH9gmAJPrAhS+5FSTSsY73WDy7eejr4gmXg==">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"yV/nSSvRBZnq2OT3F0oYktGbEOhNwJa6Hyhr/v9JWwg=">>}}]}},
    {<<"scram-sha-256">>,
     {[{<<"salt">>,<<"pjV25tLbrUUscs9JuiWhFg5IR+836ZlsAKXYwbVKJpc=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"U/4PpBqqJNLRX1VTuUwbvuhRM3f+ueg+rguYOp6kh98=">>}}]}},
    {<<"scram-sha-1">>,
     {[{<<"salt">>,<<"4RdiOMDVCo/GT05ucBVEO7J4NPM=">>},
       {<<"iterations">>,15000},
       {<<"hashes">>,
        {sanitized,<<"RwovoGCbe29f6w6atRgCqLsa8dP3ODeDIhVXpukaewo=">>}}]}}]}}]
[ns_server:debug,2025-03-28T10:13:37.623Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-03-28T10:13:37.626Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-03-28T10:13:37.626Z,ns_1@cb.local:memcached_permissions<0.603.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2025-03-28T10:13:37.631Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:maybe_apply_new_settings:704]Settings didn't change, ignoring update
[ns_server:debug,2025-03-28T10:13:37.633Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{node,'ns_1@cb.local',prometheus_auth_info}]..)
[ns_server:warn,2025-03-28T10:13:37.639Z,ns_1@cb.local:<0.971.0>:ns_memcached:connect:1460]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2025-03-28T10:13:37.639Z,ns_1@cb.local:<0.971.0>:memcached_refresh:do_refresh:171]Failed to connect to memcached: couldnt_connect_to_memcached
[error_logger:info,2025-03-28T10:13:37.642Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.978.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.643Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of rbac requested
[ns_server:debug,2025-03-28T10:13:37.644Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-03-28T10:13:37.643Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:137]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2025-03-28T10:13:37.645Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:write_cfg:156]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-03-28T10:13:37.645Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[ns_server:debug,2025-03-28T10:13:37.665Z,ns_1@cb.local:ns_ports_setup<0.790.0>:ns_ports_setup:set_children:65]Monitor ns_child_ports_sup <17009.134.0>
[ns_server:debug,2025-03-28T10:13:37.665Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:memcached_port_pid:157]ns_ports_setup seems to be ready
[ns_server:debug,2025-03-28T10:13:37.666Z,ns_1@cb.local:<0.822.0>:memcached_config_mgr:memcached_port_pid:157]ns_ports_setup seems to be ready
[ns_server:info,2025-03-28T10:13:37.667Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.980.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.668Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.980.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.670Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:find_port_pid_loop:165]Found memcached port <17009.140.0>
[ns_server:debug,2025-03-28T10:13:37.670Z,ns_1@cb.local:<0.822.0>:memcached_config_mgr:find_port_pid_loop:165]Found memcached port <17009.140.0>
[ns_server:debug,2025-03-28T10:13:37.751Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:init:93]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2025-03-28T10:13:37.754Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:init:97]activated memcached port server
[ns_server:info,2025-03-28T10:13:37.755Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:push_tls_config:234]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:warn,2025-03-28T10:13:37.757Z,ns_1@cb.local:<0.986.0>:ns_memcached:connect:1463]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:info,2025-03-28T10:13:37.762Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.987.0> on 'ns_1@cb.local'

[ns_server:debug,2025-03-28T10:13:37.763Z,ns_1@cb.local:memcached_passwords<0.591.0>:replicated_dets:select_from_table:312][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[error_logger:info,2025-03-28T10:13:37.764Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.987.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2025-03-28T10:13:37.764Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:misc:start_singleton:913]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.988.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.764Z,ns_1@cb.local:ns_orchestrator_child_sup<0.970.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.988.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.765Z,ns_1@cb.local:ns_orchestrator_sup<0.921.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.970.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.765Z,ns_1@cb.local:<0.990.0>:auto_failover:init:223]init auto_failover.
[ns_server:debug,2025-03-28T10:13:37.768Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_call:57]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2025-03-28T10:13:37.774Z,ns_1@cb.local:memcached_passwords<0.591.0>:memcached_cfg:rename_and_refresh:178]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2025-03-28T10:13:37.774Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:handle_cast:67]Refresh of isasl requested
[user:info,2025-03-28T10:13:37.788Z,ns_1@cb.local:<0.990.0>:auto_failover:enable_auto_failover:460]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2025-03-28T10:13:37.789Z,ns_1@cb.local:<0.990.0>:auto_failover:update_and_save_auto_failover_state:479]No change in timeout 120
[ns_server:debug,2025-03-28T10:13:37.789Z,ns_1@cb.local:<0.990.0>:auto_failover:update_and_save_auto_failover_state:487]No change in max count 1
[ns_server:debug,2025-03-28T10:13:37.830Z,ns_1@cb.local:<0.822.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":16,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true,\"serverGroup\":\"Group 1\"}],\"revEpoch\":1,\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"costBasedOptimizer\",\"indexAdvisor\",\"javaScriptFunctions\",\"inlineFunctions\",\"enhancedPreparedStatements\",\"readFromReplica\"],\"search\":[\"vectorSearch\",\"scopedSearchIndex\"]},\"clusterUUID\":\"\",\"clusterName\":\"\"}">>
[ns_server:warn,2025-03-28T10:13:37.832Z,ns_1@cb.local:<0.996.0>:ns_memcached:connect:1463]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:debug,2025-03-28T10:13:37.843Z,ns_1@cb.local:<0.990.0>:auto_failover:init_logic_state:249]Using auto-failover logic state {state,[],
                                    [{service_state,kv,nil,false},
                                     {service_state,n1ql,nil,false},
                                     {service_state,index,nil,false},
                                     {service_state,fts,nil,false},
                                     {service_state,cbas,nil,false},
                                     {service_state,eventing,nil,false},
                                     {service_state,backup,nil,false}],
                                    118}
[ns_server:debug,2025-03-28T10:13:37.844Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{3,63910376017}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {disable_max_count,false},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:info,2025-03-28T10:13:37.844Z,ns_1@cb.local:ns_orchestrator_sup<0.921.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.990.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.844Z,ns_1@cb.local:ns_orchestrator_sup<0.921.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.990.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.845Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.921.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.845Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([auto_failover_cfg]..)
[ns_server:info,2025-03-28T10:13:37.860Z,ns_1@cb.local:mb_master_sup<0.908.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.1004.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.860Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1004.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.861Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1005.0>},
              {id,global_tasks},
              {mfargs,{global_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.880Z,ns_1@cb.local:guardrail_enforcer<0.1007.0>:guardrail_enforcer:maybe_notify_services:137]Changed Statuses: #{}
[error_logger:info,2025-03-28T10:13:37.880Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1007.0>},
              {id,guardrail_enforcer},
              {mfargs,{guardrail_enforcer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.910Z,ns_1@cb.local:<0.1009.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2025-03-28T10:13:37.910Z,ns_1@cb.local:mb_master_sup<0.908.0>:misc:start_singleton:913]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.1009.0> on 'ns_1@cb.local'

[error_logger:info,2025-03-28T10:13:37.911Z,ns_1@cb.local:mb_master_sup<0.908.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.1009.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.911Z,ns_1@cb.local:leader_registry_sup<0.903.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.906.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.912Z,ns_1@cb.local:leader_services_sup<0.894.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.903.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:37.912Z,ns_1@cb.local:<0.893.0>:restartable:start_child:92]Started child process <0.894.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2025-03-28T10:13:37.912Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.893.0>},
              {id,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.919Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1011.0>},
              {id,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:37.920Z,ns_1@cb.local:chronicle_kv_log<0.565.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"c53116532f2f01b0da00786946759424">>,7})
[]
[error_logger:info,2025-03-28T10:13:37.923Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1013.0>},
              {id,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.924Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1014.0>},
              {id,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.930Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1015.0>},
              {id,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.941Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1016.0>},
              {id,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.962Z,ns_1@cb.local:health_monitor_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1019.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.963Z,ns_1@cb.local:health_monitor_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1021.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.964Z,ns_1@cb.local:health_monitor_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1022.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.30770475>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.973Z,ns_1@cb.local:health_monitor_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1028.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.983Z,ns_1@cb.local:health_monitor_sup<0.1018.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.1035.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:37.983Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1018.0>},
              {id,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:37.999Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1038.0>},
              {id,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:38.008Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1039.0>},
              {id,kv_hibernation_agent},
              {mfargs,{kv_hibernation_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:38.023Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1040.0>},
              {id,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:38.027Z,ns_1@cb.local:cb_creds_rotation<0.1042.0>:cb_creds_rotation:start_rotate_timer:214]Starting creds rotation timer (1800000ms)
[error_logger:info,2025-03-28T10:13:38.027Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1042.0>},
              {id,creds_rotation},
              {mfargs,{cb_creds_rotation,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:38.027Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.557.0>},
              {id,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:error,2025-03-28T10:13:38.027Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {noproc,{gen_statem,call,[mb_master,master_node,infinity]}}
    offender: [{pid,<0.832.0>},
               {id,ns_server_stats},
               {mfargs,{ns_server_stats,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:38.027Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2025-03-28T10:13:38.027Z,ns_1@cb.local:menelaus_barrier<0.299.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.292.0>
[ns_server:debug,2025-03-28T10:13:38.028Z,ns_1@cb.local:ns_server_nodes_sup<0.292.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2025-03-28T10:13:38.028Z,ns_1@cb.local:<0.291.0>:restartable:start_child:92]Started child process <0.292.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2025-03-28T10:13:38.028Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.291.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2025-03-28T10:13:38.028Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.1044.0>},
              {id,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:13:38.032Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@",admin}
[ns_server:debug,2025-03-28T10:13:38.033Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@ns_server",admin}
[error_logger:info,2025-03-28T10:13:38.038Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.1048.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:38.056Z,ns_1@cb.local:ns_server_cluster_sup<0.236.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.1049.0>},
              {id,ns_gc_runner},
              {mfargs,{ns_gc_runner,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2025-03-28T10:13:38.056Z,ns_1@cb.local:root_sup<0.215.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.236.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2025-03-28T10:13:38.059Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:debug,2025-03-28T10:13:38.059Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:147]118: Entered child_loop
[ns_server:warn,2025-03-28T10:13:38.064Z,ns_1@cb.local:<0.808.0>:ns_memcached:connect:1463]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[ns_server:warn,2025-03-28T10:13:38.130Z,ns_1@cb.local:<0.831.0>:ns_memcached:connect:1463]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2025-03-28T10:13:38.057Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2025-03-28T10:13:38.505Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.1052.0>:json_rpc_connection:init:71]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.1052.0>
[ns_server:debug,2025-03-28T10:13:38.505Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.1051.0>:json_rpc_connection:init:71]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.1051.0>
[ns_server:debug,2025-03-28T10:13:38.506Z,ns_1@cb.local:menelaus_cbauth<0.781.0>:menelaus_cbauth:handle_cast:203]Observed json rpc process {"goxdcr-cbauth",[{internal,true}],<0.1052.0>} started
[ns_server:debug,2025-03-28T10:13:38.521Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2025-03-28T10:13:38.531Z,ns_1@cb.local:ns_config_log<0.285.0>:ns_config_log:log_common:310]config change:
{metakv,<<"/query/sequences_cache/revision">>} ->
[{'_vclock',[{<<"1fb1e16f0b22f628693700f0e67dfb4b">>,{1,63910376018}}]}|
 <<"0">>]
[ns_server:debug,2025-03-28T10:13:38.570Z,ns_1@cb.local:ns_config_rep<0.657.0>:ns_config_rep:do_push_keys:385]Replicating some config keys ([{metakv,<<"/query/sequences_cache/revision">>}]..)
[ns_server:debug,2025-03-28T10:13:38.709Z,ns_1@cb.local:<0.1072.0>:memcached_refresh:do_refresh:153]Successfully connected to memcached, Trying to refresh [rbac,isasl]
[ns_server:debug,2025-03-28T10:13:38.717Z,ns_1@cb.local:memcached_refresh<0.305.0>:memcached_refresh:update_refresh_state:116]Refresh of [rbac,isasl] succeeded
[ns_server:debug,2025-03-28T10:13:38.836Z,ns_1@cb.local:<0.990.0>:auto_failover_logic:log_master_activity:130]Transitioned node {'ns_1@cb.local',<<"1fb1e16f0b22f628693700f0e67dfb4b">>} state new -> up
[ns_server:info,2025-03-28T10:13:39.017Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:push_tls_config:238]Successfully pushed TLS config to memcached
[ns_server:info,2025-03-28T10:13:39.019Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:push_tls_config:234]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2025-03-28T10:13:39.043Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:push_tls_config:238]Successfully pushed TLS config to memcached
[ns_server:debug,2025-03-28T10:13:39.056Z,ns_1@cb.local:ns_gc_runner<0.1049.0>:ns_gc_runner:handle_info:125]GC populating new pid list of size=593, prevMaxGcDuration=0 us
[ns_server:debug,2025-03-28T10:13:44.153Z,ns_1@cb.local:compiled_roles_cache<0.448.0>:menelaus_roles:build_compiled_roles:1213]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2025-03-28T10:14:06.785Z,ns_1@cb.local:cb_saml<0.430.0>:cb_saml:refresh_metadata:526]Refreshing metadata
[ns_server:debug,2025-03-28T10:14:06.785Z,ns_1@cb.local:cb_saml<0.430.0>:cb_saml:restart_refresh_timer:583]Disabling refresh timer
[ns_server:debug,2025-03-28T10:14:07.251Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:14:07.251Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:14:07.251Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:14:07.251Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:14:37.252Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:14:37.253Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:14:37.253Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:14:37.253Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:14:41.628Z,ns_1@cb.local:ldap_auth_cache<0.410.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2025-03-28T10:15:07.253Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:15:07.253Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:15:07.254Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_daemon:process_scheduler_message:1316]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2025-03-28T10:15:07.254Z,ns_1@cb.local:compaction_daemon<0.879.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2025-03-28T10:15:24.656Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.1052.0>:json_rpc_connection:handle_info:142]Socket closed
[ns_server:debug,2025-03-28T10:15:24.656Z,ns_1@cb.local:<0.1061.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.749.0>} exited with reason normal
[ns_server:info,2025-03-28T10:15:24.656Z,ns_1@cb.local:menelaus_cbauth_worker-goxdcr-cbauth<0.1059.0>:menelaus_cbauth_worker:handle_info:93]Observed json rpc process <0.1052.0> died with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.656Z,ns_1@cb.local:<0.1056.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1052.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.656Z,ns_1@cb.local:menelaus_cbauth<0.781.0>:menelaus_cbauth:handle_info:258]Observed worker process {worker,<0.1059.0>,"goxdcr-cbauth",internal,
                                #Ref<0.3437536922.3327655938.12736>,
                                <0.1052.0>} died with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.657Z,ns_1@cb.local:<0.1130.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1057.0>} exited with reason normal
[ns_server:debug,2025-03-28T10:15:24.657Z,ns_1@cb.local:<0.1119.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1006.0>} exited with reason normal
[ns_server:debug,2025-03-28T10:15:24.657Z,ns_1@cb.local:<0.1113.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.886.0>} exited with reason normal
[ns_server:debug,2025-03-28T10:15:24.657Z,ns_1@cb.local:<0.1073.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.753.0>} exited with reason normal
[ns_server:debug,2025-03-28T10:15:24.697Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.1051.0>:json_rpc_connection:handle_info:142]Socket closed
[ns_server:debug,2025-03-28T10:15:24.697Z,ns_1@cb.local:<0.1055.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1051.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.986Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:151]118: Got EOL
[ns_server:info,2025-03-28T10:15:24.986Z,ns_1@cb.local:<0.9.0>:ns_bootstrap:stop:36]Initiated server shutdown
[error_logger:info,2025-03-28T10:15:24.986Z,ns_1@cb.local:<0.9.0>:ale_error_logger_handler:do_log:101]Initiated server shutdown
[ns_server:debug,2025-03-28T10:15:24.986Z,ns_1@cb.local:<0.1050.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1049.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.987Z,ns_1@cb.local:<0.1043.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1042.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.987Z,ns_1@cb.local:<0.1041.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1040.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.990Z,ns_1@cb.local:<0.982.0>:remote_monitors:monitor_loop:122]Monitored remote process <17009.140.0> went down with: shutdown
[ns_server:debug,2025-03-28T10:15:24.991Z,ns_1@cb.local:<0.983.0>:remote_monitors:monitor_loop:122]Monitored remote process <17009.140.0> went down with: shutdown
[ns_server:debug,2025-03-28T10:15:24.991Z,ns_1@cb.local:<0.979.0>:remote_monitors:monitor_loop:122]Monitored remote process <17009.134.0> went down with: shutdown
[ns_server:debug,2025-03-28T10:15:24.991Z,ns_1@cb.local:ns_ports_setup<0.790.0>:ns_ports_setup:children_loop_continue:101]ns_child_ports_sup <17009.134.0> died on babysitter node with shutdown. Restart.
[ns_server:debug,2025-03-28T10:15:24.992Z,ns_1@cb.local:memcached_config_mgr<0.817.0>:memcached_config_mgr:handle_info:199]Got DOWN with reason: shutdown from memcached port server: <17009.140.0>. Shutting down
[error_logger:error,2025-03-28T10:15:24.992Z,ns_1@cb.local:<0.816.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {<0.816.0>,suppress_max_restart_intensity}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<17009.140.0>,shutdown}}
    offender: [{pid,<0.817.0>},
               {id,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-03-28T10:15:24.993Z,ns_1@cb.local:memcached_config_mgr<0.4755.0>:memcached_config_mgr:memcached_port_pid:155]waiting for completion of initial ns_ports_setup round
[error_logger:info,2025-03-28T10:15:24.993Z,ns_1@cb.local:<0.816.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.816.0>,suppress_max_restart_intensity}
    started: [{pid,<0.4755.0>},
              {id,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2025-03-28T10:15:24.993Z,ns_1@cb.local:<0.822.0>:terse_cluster_info_uploader:handle_info:64]Got DOWN with reason: shutdown from memcached port server: <17009.140.0>. Shutting down
[ns_server:debug,2025-03-28T10:15:24.993Z,ns_1@cb.local:<0.823.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.822.0>} exited with reason {shutdown,
                                                                                               {memcached_port_server_down,
                                                                                                <17009.140.0>,
                                                                                                shutdown}}
[ns_server:debug,2025-03-28T10:15:24.993Z,ns_1@cb.local:<0.984.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.817.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <17009.140.0>,
                                                                                 shutdown}}
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1037.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1035.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1036.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1035.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1030.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1028.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1029.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1028.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1023.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1022.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1020.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.1019.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1017.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {master_activity_events,<0.1016.0>} exited with reason killed
[ns_server:info,2025-03-28T10:15:24.999Z,ns_1@cb.local:mb_master<0.906.0>:mb_master:terminate:247]Synchronously shutting down child mb_master_sup
[ns_server:debug,2025-03-28T10:15:24.999Z,ns_1@cb.local:<0.1012.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.1011.0>} exited with reason shutdown
[ns_server:info,2025-03-28T10:15:25.000Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.1009.0> registered as 'license_reporting' terminated.
[ns_server:debug,2025-03-28T10:15:25.000Z,ns_1@cb.local:<0.1008.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1007.0>} exited with reason shutdown
[ns_server:info,2025-03-28T10:15:25.000Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.1004.0> registered as 'tombstone_purger' terminated.
[ns_server:debug,2025-03-28T10:15:25.000Z,ns_1@cb.local:<0.1010.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.1009.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.000Z,ns_1@cb.local:<0.992.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.990.0>} exited with reason shutdown
[ns_server:info,2025-03-28T10:15:25.000Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.990.0> registered as 'auto_failover' terminated.
[ns_server:info,2025-03-28T10:15:25.000Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.988.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2025-03-28T10:15:25.000Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.987.0> registered as 'auto_rebalance' terminated.
[ns_server:info,2025-03-28T10:15:25.001Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.980.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2025-03-28T10:15:25.000Z,ns_1@cb.local:<0.991.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.990.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.004Z,ns_1@cb.local:<0.920.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.919.0>} exited with reason shutdown
[ns_server:info,2025-03-28T10:15:25.004Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.919.0> registered as 'chronicle_master' terminated.
[ns_server:info,2025-03-28T10:15:25.005Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_down:286]Process <0.918.0> registered as 'ns_tick' terminated.
[ns_server:debug,2025-03-28T10:15:25.005Z,ns_1@cb.local:leader_activities<0.896.0>:leader_activities:handle_internal_process_down:440]Process {quorum_nodes_manager,<0.912.0>} terminated with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.005Z,ns_1@cb.local:<0.915.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.912.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.005Z,ns_1@cb.local:leader_activities<0.896.0>:leader_activities:handle_internal_process_down:440]Process {acquirer,<0.909.0>} terminated with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.005Z,ns_1@cb.local:leader_lease_agent<0.898.0>:leader_lease_agent:handle_abolish_lease:254]Received abolish lease request from {lease_holder,
                                     <<"c33e2fe123e7f3070ea3e5d98a698a27">>,
                                     'ns_1@cb.local'} when lease is {lease,
                                                                     {lease_holder,
                                                                      <<"c33e2fe123e7f3070ea3e5d98a698a27">>,
                                                                      'ns_1@cb.local'},
                                                                     -576460619917793666,
                                                                     -576460604917793666,
                                                                     {timer,
                                                                      #Ref<0.3437536922.3327655937.19112>,
                                                                      {lease_expired,
                                                                       {lease_holder,
                                                                        <<"c33e2fe123e7f3070ea3e5d98a698a27">>,
                                                                        'ns_1@cb.local'}}},
                                                                     active}
[ns_server:debug,2025-03-28T10:15:25.005Z,ns_1@cb.local:leader_registry<0.904.0>:leader_registry:handle_new_leader:275]New leader is undefined. Invalidating name cache.
[ns_server:debug,2025-03-28T10:15:25.005Z,ns_1@cb.local:<0.910.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.909.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.006Z,ns_1@cb.local:leader_lease_agent<0.898.0>:leader_lease_agent:handle_abolish_lease:259]Expiring abolished lease
[ns_server:debug,2025-03-28T10:15:25.006Z,ns_1@cb.local:<0.907.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.906.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.006Z,ns_1@cb.local:<0.905.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.904.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.012Z,ns_1@cb.local:leader_activities<0.896.0>:leader_activities:handle_internal_process_down:440]Process {agent,<0.898.0>} terminated with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.013Z,ns_1@cb.local:<0.893.0>:restartable:shutdown_child:114]Successfully terminated process <0.894.0>
[ns_server:debug,2025-03-28T10:15:25.013Z,ns_1@cb.local:<0.891.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.890.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.013Z,ns_1@cb.local:<0.880.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.879.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.014Z,ns_1@cb.local:<0.865.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.863.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.014Z,ns_1@cb.local:<0.857.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.856.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.014Z,ns_1@cb.local:<0.867.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.866.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.015Z,ns_1@cb.local:<0.864.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.863.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.015Z,ns_1@cb.local:<0.859.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.856.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.015Z,ns_1@cb.local:<0.854.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.852.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.844.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.843.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.853.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.852.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.840.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.839.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.838.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.837.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.829.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.828.0>} exited with reason shutdown
[error_logger:error,2025-03-28T10:15:25.017Z,ns_1@cb.local:ns_server_sup<0.557.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: shutdown_error
    reason: {shutdown,{memcached_port_server_down,<17009.140.0>,shutdown}}
    offender: [{pid,<0.822.0>},
               {id,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.1046.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ale_stats_events,<0.1044.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.018Z,ns_1@cb.local:<0.806.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.805.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.018Z,ns_1@cb.local:<0.800.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.799.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.019Z,ns_1@cb.local:<0.796.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.795.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.019Z,ns_1@cb.local:<0.792.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.790.0>} exited with reason killed
[ns_server:debug,2025-03-28T10:15:25.019Z,ns_1@cb.local:<0.791.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.790.0>} exited with reason killed
[ns_server:info,2025-03-28T10:15:25.020Z,ns_1@cb.local:menelaus_cbauth<0.781.0>:menelaus_cbauth:terminate_external_connections:159]External connections to be terminated: []
[ns_server:debug,2025-03-28T10:15:25.020Z,ns_1@cb.local:<0.783.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.781.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.020Z,ns_1@cb.local:<0.786.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ssl_service_events,<0.781.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.021Z,ns_1@cb.local:<0.785.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.781.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.021Z,ns_1@cb.local:<0.784.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.781.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.022Z,ns_1@cb.local:<0.782.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {json_rpc_events,<0.781.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.024Z,ns_1@cb.local:<0.779.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.778.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.030Z,ns_1@cb.local:<0.742.0>:restartable:shutdown_child:114]Successfully terminated process <0.743.0>
[ns_server:debug,2025-03-28T10:15:25.016Z,ns_1@cb.local:<0.842.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_tick_event,<0.841.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.035Z,ns_1@cb.local:<0.714.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.713.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.038Z,ns_1@cb.local:<0.712.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.711.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.041Z,ns_1@cb.local:<0.710.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.709.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.042Z,ns_1@cb.local:<0.708.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.707.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.042Z,ns_1@cb.local:<0.699.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.698.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.042Z,ns_1@cb.local:<0.692.0>:restartable:shutdown_child:114]Successfully terminated process <0.696.0>
[ns_server:debug,2025-03-28T10:15:25.042Z,ns_1@cb.local:<0.690.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {buckets_events,<0.689.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.034Z,ns_1@cb.local:<0.723.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.722.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.102Z,ns_1@cb.local:chronicle_kv_log<0.565.0>:chronicle_kv_log:log:59]update (key: tasks, rev: {<<"c53116532f2f01b0da00786946759424">>,14})
[]
[error_logger:error,2025-03-28T10:15:25.103Z,ns_1@cb.local:bucket_info_cache_invalidations<0.676.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: gen_event:init_it/6
    pid: <0.676.0>
    registered_name: bucket_info_cache_invalidations
    exception exit: killed
      in function  gen_event:terminate_server/4 (gen_event.erl, line 580)
    ancestors: [bucket_info_cache,ns_server_sup,ns_server_nodes_sup,
                  <0.291.0>,ns_server_cluster_sup,root_sup,<0.155.0>]
    message_queue_len: 0
    messages: []
    links: []
    dictionary: []
    trap_exit: true
    status: running
    heap_size: 376
    stack_size: 28
    reductions: 510
  neighbours:

[ns_server:debug,2025-03-28T10:15:25.103Z,ns_1@cb.local:<0.677.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.675.0>} exited with reason killed
[ns_server:debug,2025-03-28T10:15:25.103Z,ns_1@cb.local:<0.671.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.669.0>} exited with reason killed
[ns_server:debug,2025-03-28T10:15:25.103Z,ns_1@cb.local:<0.668.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.667.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.104Z,ns_1@cb.local:<0.674.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.673.0>} exited with reason killed
[ns_server:debug,2025-03-28T10:15:25.104Z,ns_1@cb.local:<0.660.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.657.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.104Z,ns_1@cb.local:<0.659.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.657.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.104Z,ns_1@cb.local:<0.658.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events_local,<0.657.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.105Z,ns_1@cb.local:<0.605.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.603.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.105Z,ns_1@cb.local:<0.604.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.603.0>} exited with reason shutdown
[ns_server:info,2025-03-28T10:15:25.105Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:terminate:609]Terminate: shutdown
[ns_server:debug,2025-03-28T10:15:25.105Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:terminate_prometheus:773]Terminating Prometheus gracefully
[ns_server:debug,2025-03-28T10:15:25.106Z,ns_1@cb.local:<0.593.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.591.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.106Z,ns_1@cb.local:<0.592.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.591.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.106Z,ns_1@cb.local:<0.611.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.610.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.128Z,ns_1@cb.local:prometheus-goport<0.590.0>:goport:handle_eof:592]Stream 'stderr' closed
[ns_server:debug,2025-03-28T10:15:25.129Z,ns_1@cb.local:prometheus-goport<0.590.0>:goport:handle_eof:592]Stream 'stdout' closed
[ns_server:info,2025-03-28T10:15:25.129Z,ns_1@cb.local:prometheus-goport<0.590.0>:goport:handle_process_exit:573]Port exited with status 0.
[ns_server:info,2025-03-28T10:15:25.135Z,ns_1@cb.local:<0.584.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port prometheus. Exiting normally
[ns_server:debug,2025-03-28T10:15:25.135Z,ns_1@cb.local:prometheus_cfg<0.578.0>:prometheus_cfg:terminate_prometheus:794]Prometheus port server stopped successfully
[ns_server:debug,2025-03-28T10:15:25.135Z,ns_1@cb.local:<0.579.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.578.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.136Z,ns_1@cb.local:<0.572.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.571.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.172Z,ns_1@cb.local:<0.570.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.569.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.206Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:terminate:198]Shutting down port ns_couchdb
[ns_server:debug,2025-03-28T10:15:25.206Z,ns_1@cb.local:<0.566.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_kv_event_manager,<0.565.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.206Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:port_shutdown:348]Shutdown command: "shutdown"
[ns_server:debug,2025-03-28T10:15:25.207Z,ns_1@cb.local:<0.556.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.473.0> died with shutdown. Exiting
[ns_server:debug,2025-03-28T10:15:25.219Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.11634>,
                               inet_tcp_dist,<0.553.0>,
                               #Ref<0.3437536922.3327655938.11636>}
[error_logger:info,2025-03-28T10:15:25.219Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',#Port<0.54>,normal}}
[error_logger:info,2025-03-28T10:15:25.219Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.553.0>,connection_closed}}
[error_logger:info,2025-03-28T10:15:25.219Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9011397,#Ref<0.3437536922.3327787009.12471>}}}
[ns_server:debug,2025-03-28T10:15:25.220Z,ns_1@cb.local:net_kernel<0.230.0>:cb_dist:info_msg:1098]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2025-03-28T10:15:25.220Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Added connection {con,#Ref<0.3437536922.3327655938.18736>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2025-03-28T10:15:25.220Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Updated connection: {con,#Ref<0.3437536922.3327655938.18736>,
                                  inet_tcp_dist,<0.4783.0>,
                                  #Ref<0.3437536922.3327655938.18739>}
[error_logger:info,2025-03-28T10:15:25.223Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.4783.0>,{recv_status_failed,{error,closed}}}}
[ns_server:debug,2025-03-28T10:15:25.223Z,ns_1@cb.local:cb_dist<0.228.0>:cb_dist:info_msg:1098]cb_dist: Connection down: {con,#Ref<0.3437536922.3327655938.18736>,
                               inet_tcp_dist,<0.4783.0>,
                               #Ref<0.3437536922.3327655938.18739>}
[error_logger:info,2025-03-28T10:15:25.224Z,ns_1@cb.local:net_kernel<0.230.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1411,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:info,2025-03-28T10:15:25.227Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:handle_info:149]Got {exit_status,0} from port ns_couchdb. Exiting normally
[ns_server:debug,2025-03-28T10:15:25.227Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:terminate:201]ns_couchdb has exited
[ns_server:info,2025-03-28T10:15:25.227Z,ns_1@cb.local:ns_couchdb_port<0.472.0>:ns_port_server:log:226]ns_couchdb<0.472.0>: 234: got shutdown request. Exiting
ns_couchdb<0.472.0>: [os_mon] cpu supervisor port (cpu_sup): Erlang has closed
ns_couchdb<0.472.0>: [os_mon] memory supervisor port (memsup): Erlang has closed

[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.453.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.451.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.454.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.451.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.452.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.451.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.449.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {user_storage_events,<0.448.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.437.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.436.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.450.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.448.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.431.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.430.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.228Z,ns_1@cb.local:<0.411.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.410.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.229Z,ns_1@cb.local:<0.328.0>:restartable:shutdown_child:114]Successfully terminated process <0.389.0>
[ns_server:debug,2025-03-28T10:15:25.229Z,ns_1@cb.local:<0.310.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.309.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.230Z,ns_1@cb.local:<0.296.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.293.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.230Z,ns_1@cb.local:<0.297.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.293.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.230Z,ns_1@cb.local:<0.291.0>:restartable:shutdown_child:114]Successfully terminated process <0.292.0>
[ns_server:debug,2025-03-28T10:15:25.230Z,ns_1@cb.local:<0.286.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.285.0>} exited with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.230Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:terminate:859]Config is terminating with reason shutdown
[ns_server:debug,2025-03-28T10:15:25.231Z,ns_1@cb.local:ns_config<0.281.0>:ns_config:wait_saver:844]Done waiting for saver.
[ns_server:debug,2025-03-28T10:15:25.232Z,ns_1@cb.local:<0.275.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_external_events,<0.274.0>} exited with reason shutdown
[error_logger:info,2025-03-28T10:15:25.235Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    application: ns_server
    exited: stopped
    type: permanent

[ns_server:info,2025-03-28T10:15:25.235Z,ns_1@cb.local:<0.9.0>:ns_bootstrap:stop:40]Successfully stopped ns_server
